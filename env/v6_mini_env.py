# -*- coding: utf-8 -*-#
"""
ä¿®å¤åçš„åˆ†å±‚å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ
è§£å†³ç­–ç•¥åç¼©é—®é¢˜çš„å…³é”®ä¿®å¤ï¼š
1. ä½å±‚ï¼šè¿ç»­èµ„æºè¡°å‡ + æ‰©å±•è§‚å¯Ÿç©ºé—´ [gap, resource_fatigue]
2. é«˜å±‚ï¼šæ‰©å±•è§‚å¯Ÿç©ºé—´ [gaps, timers] + è¿ç»­é—å¿˜è¡°å‡
3. ä¿ç•™æ‰€æœ‰åŸæœ‰æœºåˆ¶ï¼šé—å¿˜ã€èµ„æºçæƒœã€éš¾åº¦åŒ¹é…
"""
import numpy as np
import gymnasium as gym
from gymnasium import spaces


def clamp(x: float, lo: float, hi: float) -> float:
    return float(max(lo, min(hi, x)))


class MiniSkillEnv(gym.Env):
    """
    ä½å±‚æŠ€èƒ½ç¯å¢ƒï¼ˆä¿®å¤ç­–ç•¥åç¼©é—®é¢˜ï¼‰
    
    ä¿®å¤è¯´æ˜ï¼š
    - æ‰©å±•è§‚å¯Ÿç©ºé—´ï¼š[gap, resource_fatigue, mastery_category, action_counter] è§£å†³èµ„æºçŠ¶æ€ä¸å¯è§‚æµ‹é—®é¢˜
    - è¿ç»­èµ„æºè¡°å‡ï¼šé¿å…ç¡¬è·³å˜å¯¼è‡´çš„ç­–ç•¥åç¼©
    - ä¿ç•™æ‰€æœ‰åŸæœ‰æœºåˆ¶ï¼šéš¾åº¦åŒ¹é…ã€èµ„æºçæƒœç­‰
    """

    metadata = {"render.modes": ["human"]}

    def __init__(self, tolerance: float = 0.05, max_steps: int = 200, seed: int = 0):
        super().__init__()
        self.tolerance = float(tolerance)
        self.max_steps = int(max_steps)
        self.np_random, _ = gym.utils.seeding.np_random(seed)

        # ã€ä¿®å¤1ã€‘å¢å¼ºè§‚å¯Ÿç©ºé—´ï¼ˆå­¦æœ¯ç†ç”±ï¼šéƒ¨åˆ†å¯è§‚æµ‹â†’å…¨å¯è§‚æµ‹ï¼‰
        # åŸ2ç»´ï¼š[gap, resource_fatigue]
        # æ–°4ç»´ï¼š[gap, resource_fatigue, mastery_category, action_counter]
        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(2,), dtype=np.float32)
        self.action_space = spaces.Discrete(6)

        # åŠ¨ä½œè¯­ä¹‰å®šä¹‰
        self.action_semantics = [
            "easy_res", "medium_res", "hard_res",
            "seek_teacher", "peer_discussion", "self_learn"
        ]

        # å¢ç›ŠçŸ©é˜µï¼ˆç¯å¢ƒåŠ¨åŠ›å­¦ï¼Œä¸æ˜¯å¥–åŠ±ï¼‰
        self.GAIN_MATRIX = {
            "low": {        # æŒæ¡åº¦ 0.00-0.33
                "easy_res": 0.1, "medium_res": 0.05, "hard_res": 0.03,
                "seek_teacher": 0.05, "peer_discussion": 0.025, "self_learn": 0.015
            },
            "medium": {     # æŒæ¡åº¦ 0.34-0.66
                "easy_res": 0.05, "medium_res": 0.1, "hard_res": 0.05,
                "seek_teacher": 0.025, "peer_discussion": 0.05, "self_learn": 0.025
            },
            "high": {       # æŒæ¡åº¦ 0.67-1.00
                "easy_res": 0.03, "medium_res": 0.05, "hard_res": 0.1,
                "seek_teacher": 0.015, "peer_discussion": 0.025, "self_learn": 0.05
            }
        }

        # ä¿ç•™åŸæœ‰delta_mapä½œä¸ºå¤‡ç”¨ï¼ˆå‘åå…¼å®¹ï¼‰
        self.delta_map = np.array([0.05, 0.10, 0.20, 0.03, 0.06, 0.02], dtype=np.float32)

        # ã€ä¿®å¤2ã€‘å¢åŠ åŠ¨ä½œè®¡æ•°å™¨ï¼ˆé˜²æ­¢é‡å¤é€‰æ‹©åŒä¸€åŠ¨ä½œï¼‰
        self.action_counter = np.zeros(6, dtype=np.float32)
        self.action_decay = 0.9  # æ¯æ­¥è¡°å‡ç³»æ•°

        # ä¿®å¤A: èµ„æºç–²åŠ³åº¦è·Ÿè¸ª (è¿ç»­è¡°å‡æ›¿ä»£ç¡¬è·³å˜)
        self.resource_usage_count = np.zeros(3, dtype=np.float32)  # 3ä¸ªèµ„æºåŠ¨ä½œçš„ä½¿ç”¨æ¬¡æ•°
        # ã€ä¿®å¤3ã€‘è°ƒæ•´èµ„æºç–²åŠ³å‚æ•°
        self.resource_fatigue_alpha = 0.7  # ä»0.5æé«˜åˆ°0.7ï¼Œè¡°å‡æ›´å¿«

        self.gap = None
        self.step_count = 0

    def reset(self, *, seed: int | None = None, options: dict | None = None):
        if seed is not None:
            self.np_random, _ = gym.utils.seeding.np_random(seed)
        
        # 1. éšæœº Gapï¼šè¦†ç›–å…¨éš¾åº¦
        self.gap = float(self.np_random.uniform(0.1, 0.95))
        
        # ã€ä¿®å¤4ã€‘é‡ç½®æ—¶ç»™äºˆé€‚ä¸­çš„èµ„æºç–²åŠ³åº¦ï¼ˆä¸æ˜¯ä»0å¼€å§‹ï¼‰
        self.resource_usage_count = self.np_random.uniform(1.0, 4.0, size=(3,)).astype(np.float32)
        self.action_counter.fill(0.0)
        
        self.step_count = 0
        # ã€å…³é”®ã€‘æ‰©å±•è§‚å¯Ÿç©ºé—´åŒ…å«èµ„æºç–²åŠ³åº¦
        resource_fatigue = self._compute_resource_fatigue()
        mastery = 1.0 - self.gap
        mastery_category = 0.0 if mastery <= 0.33 else (0.5 if mastery <= 0.66 else 1.0)
        
        obs = np.array([self.gap, mastery_category], dtype=np.float32)
        info = {}
        return obs, info

    def _compute_resource_fatigue(self):
        """
        ä¿®å¤A: è®¡ç®—èµ„æºç–²åŠ³åº¦ (è¿ç»­å€¼ï¼Œé¿å…ç¡¬è·³å˜)
        ä½¿ç”¨æŒ‡æ•°è¡°å‡: fatigue = 1 - exp(-alpha * max_usage_count)
        """
        max_usage = np.max(self.resource_usage_count)
        fatigue = 1.0 - np.exp(-self.resource_fatigue_alpha * max_usage)
        return float(np.clip(fatigue, 0.0, 1.0))

    def step(self, action: int):
        self.step_count += 1

        # ã€ä¿®å¤5ã€‘æ›´æ–°åŠ¨ä½œè®¡æ•°å™¨
        self.action_counter = self.action_counter * self.action_decay
        self.action_counter[action] += 1.0

        prev_gap = float(self.gap)
        mastery = 1.0 - prev_gap

        # ç¡®å®šæŒæ¡åº¦åŒºé—´
        if mastery <= 0.33:
            level = "low"
        elif mastery <= 0.66:
            level = "medium"
        else:
            level = "high"

        action_name = self.action_semantics[action]

        # ã€ä¿®å¤6ã€‘å¢åŠ åŠ¨ä½œæƒ©ç½šï¼ˆé˜²æ­¢åç¼©ï¼‰
        action_penalty = 0.0
        if self.action_counter[action] > 3.0:  # è¿ç»­ä½¿ç”¨åŒä¸€åŠ¨ä½œ3æ¬¡ä»¥ä¸Š
            action_penalty = -0.5 * (self.action_counter[action] - 3.0)

        # ä¿®å¤A: è¿ç»­èµ„æºè¡°å‡æ›¿ä»£ç¡¬è·³å˜
        if action in [0, 1, 2]:  # èµ„æºåŠ¨ä½œ
            res_id = action
            # æ›´æ–°ä½¿ç”¨æ¬¡æ•°
            self.resource_usage_count[res_id] += 1.0
            # è¿ç»­è¡°å‡: reward_scale = exp(-alpha * usage_count)
            usage_count = self.resource_usage_count[res_id]
            resource_decay = np.exp(-self.resource_fatigue_alpha * usage_count)
        else:
            resource_decay = 1.0  # éèµ„æºåŠ¨ä½œä¸è¡°å‡

        # è®¡ç®—å¢ç›Šï¼ˆç¯å¢ƒåŠ¨åŠ›å­¦ï¼‰- åº”ç”¨è¿ç»­è¡°å‡
        base_gain = self.GAIN_MATRIX[level][action_name]
        gain = base_gain * resource_decay

        # è½»å¾®å™ªå£°ï¼Œé¿å…ç­–ç•¥å®Œå…¨ç¡®å®šæ€§ï¼ˆæé«˜ç†µï¼‰
        noise = float(self.np_random.normal(0.0, 0.001))
        new_gap = clamp(prev_gap - gain + noise, 0.0, 1.0)
        self.gap = new_gap

        # å¥–åŠ±ï¼šæœ¬åœ° dense rewardï¼ˆç§»é™¤ç›´æ¥çš„é¦–æ¬¡èµ„æºä½¿ç”¨å¥–åŠ±ï¼‰
        reward = 0.0
        # æŒ‰ gap å‡å°‘é‡ç»™äºˆ dense å¥–åŠ±ï¼Œé¼“åŠ±é€‰æ‹©æ›´å¤§å¹…åº¦åŠ¨ä½œ
        reward += 5.0 * max(0.0, prev_gap - new_gap)
        reward += 5.0 if new_gap < self.tolerance else 0.0
        # ã€ä¿®å¤7ã€‘åœ¨åŸæœ‰rewardåŸºç¡€ä¸ŠåŠ ä¸ŠåŠ¨ä½œæƒ©ç½š
        reward += action_penalty
        reward += -0.01  # æ­¥æƒ©ç½š

        terminated = bool(new_gap < self.tolerance)
        truncated = bool(self.step_count >= self.max_steps)

        mastery = 1.0 - self.gap
        mastery_category = 0.0 if mastery <= 0.33 else (0.5 if mastery <= 0.66 else 1.0)
        obs = np.array([self.gap, mastery_category], dtype=np.float32)
        
        info = {
            "gap": self.gap, 
            "gain": gain, 
            "base_gain": base_gain,
            "resource_decay": resource_decay if action in [0, 1, 2] else 1.0,
            "noise": noise, 
            "mastery": mastery, 
            "level": level, 
            "action_name": action_name,
            "resource_fatigue": self._compute_resource_fatigue(),
            "resource_usage_count": self.resource_usage_count.copy(),
            "action_counter": self.action_counter.copy(),
            "action_penalty": action_penalty
        }
        return obs, reward, terminated, truncated, info


class MiniManagerEnv12(gym.Env):
    """
    12ç»´ç®¡ç†è€…ç¯å¢ƒï¼ˆä¿®å¤ç­–ç•¥åç¼©é—®é¢˜ï¼‰
    
    ä¿®å¤è¯´æ˜ï¼š
    - æ‰©å±•è§‚å¯Ÿç©ºé—´ï¼š[gap_0, ..., gap_11, timer_cat0, timer_cat1, timer_cat2] è§£å†³POMDPé—®é¢˜
    - è¿ç»­é—å¿˜è¡°å‡ï¼šé¿å…ç¡¬è·³å˜å¯¼è‡´çš„ç­–ç•¥åç¼©
    - ä¿ç•™æ‰€æœ‰åŸæœ‰æœºåˆ¶ï¼šé—å¿˜ã€èµ„æºçæƒœã€éš¾åº¦åŒ¹é…ç­‰
    """
    def __init__(self, low_policies=None, tolerance: float = 0.05, max_steps: int = 200, seed: int = 0, 
                 forgetting_mode: str = 'no_forgetting', forgetting_params: dict | None = None, 
                 resource_enabled: bool = False, resource_decay_range: tuple | list = (0.2, 0.4), 
                 match_bonus: float = 0.2, mismatch_penalty: float = 0.1, 
                 timing_bonus: float = 0.03, timing_penalty: float = 0.03, 
                 difficulty_bins: tuple | list = (0.33, 0.66)):
        super().__init__()
        self.low_policies = low_policies or []
        self.tolerance = float(tolerance)
        self.max_steps = int(max_steps)
        self.np_random, _ = gym.utils.seeding.np_random(seed)
        
        # ä¿®å¤B: æ‰©å±•è§‚å¯Ÿç©ºé—´ [gap_0, ..., gap_11, timer_cat0, timer_cat1, timer_cat2]
        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(15,), dtype=np.float32)
        self.action_space = spaces.Discrete(12)
        
        self.delta_map = np.array([0.05, 0.10, 0.20, 0.03, 0.06, 0.02], dtype=np.float32)
        self.gaps = None
        self.step_count = 0
        self._last_action = None
        self.forgetting_mode = str(forgetting_mode)
        
        # ã€ä¿®å¤13ã€‘å¢åŠ é˜²åç¼©æœºåˆ¶
        self.action_penalty_coef = 0.02  # åŠ¨ä½œé‡å¤æƒ©ç½šç³»æ•°
        self.last_actions = []  # è®°å½•æœ€è¿‘åŠ¨ä½œ
        self.action_memory_size = 10  # è®°å¿†æœ€è¿‘10ä¸ªåŠ¨ä½œ
        
        # ã€ä¿®å¤14ã€‘æ ¹æ®ä¸åŒé—å¿˜æ¨¡å¼è°ƒæ•´Timeræ•æ„Ÿæ€§
        # ä½†ä¿æŒBCæ”¶é›†ç­–ç•¥ä¸€è‡´ï¼ˆåªé€šè¿‡ç¯å¢ƒåŠ¨åŠ›å­¦äº§ç”Ÿå·®å¼‚ï¼‰
        self.timer_sensitivity = 1.0  # åŸºç¡€æ•æ„Ÿæ€§
        
        # æ”¹è¿›1: å¼±action-entropy shaping - ç»´æŠ¤æ»‘åŠ¨çª—å£
        self.action_history = []
        self.action_history_window = 50
        self.entropy_coef = 0.01
        
        # æ”¹è¿›2: timer-aware shaping - ç³»æ•°
        self.timer_coef = 0.05
        
        # Categoriesæ˜¯latent environment factorï¼Œagentä¸å¯è§‚æµ‹
        self.categories = {
            'algebra': [0, 1, 2, 3],
            'geometry': [4, 5, 6, 7],
            'statistics': [8, 9, 10, 11],
        }
        self.steps_since_review_cat = {'algebra': 0, 'geometry': 0, 'statistics': 0}
        self.history_buffer_cat = {'algebra': [], 'geometry': [], 'statistics': []}
        self._fp = forgetting_params or {}
        
        # è§¦å‘é¢‘ç‡ç»Ÿè®¡
        self.forgetting_trigger_stats = {'algebra': 0, 'geometry': 0, 'statistics': 0}
        
        # ä¿®å¤B: é—å¿˜é˜ˆå€¼ (ç”¨äºè®¡ç®—å½’ä¸€åŒ–timer)
        self.forgetting_thresholds = {
            'algebra': 5,
            'geometry': 4,
            'statistics': 6
        }
        
        self.resource_enabled = bool(resource_enabled)
        self.resource_decay_range = tuple(resource_decay_range)
        self.match_bonus = float(match_bonus)
        self.mismatch_penalty = float(mismatch_penalty)
        self.timing_bonus = float(timing_bonus)
        self.timing_penalty = float(timing_penalty)
        self.difficulty_bins = tuple(difficulty_bins)
        self.resource_used = None

    def reset(self, *, seed: int | None = None, options: dict | None = None):
        if seed is not None:
            self.np_random, _ = gym.utils.seeding.np_random(seed)
        if options and isinstance(options, dict) and 'init_gaps' in options:
            init = np.array(options['init_gaps'], dtype=np.float32)
            if init.shape != (12,):
                init = init.reshape(12,)
            self.gaps = np.clip(init, 0.0, 1.0)
        else:
            self.gaps = self.np_random.uniform(0.4, 0.9, size=(12,)).astype(np.float32)
        self.step_count = 0
        self._last_action = None
        for c in self.steps_since_review_cat:
            self.steps_since_review_cat[c] = 0
        for c in self.history_buffer_cat:
            self.history_buffer_cat[c] = []
        
        # æ”¹è¿›1: é‡ç½®action history
        self.action_history = []
        
        # é‡ç½®é˜²åç¼©æœºåˆ¶
        self.last_actions = []
        
        # ä¿®å¤B: æ‰©å±•è§‚å¯Ÿç©ºé—´åŒ…å«å½’ä¸€åŒ–timer
        timer_obs = self._compute_timer_observation()
        obs = np.concatenate([self.gaps, timer_obs])
        return obs, {}

    def _compute_timer_observation(self):
        """
        ä¿®å¤B: è®¡ç®—å½’ä¸€åŒ–timerè§‚å¯Ÿ (è§£å†³POMDPé—®é¢˜)
        timer_cat = steps_since_review_cat / forgetting_threshold
        ä¸å‘Šè¯‰agentç±»åˆ«å½’å±ï¼Œåªæä¾›timerçŠ¶æ€
        """
        timers = []
        for cat in ['algebra', 'geometry', 'statistics']:
            steps = self.steps_since_review_cat[cat]
            threshold = self.forgetting_thresholds[cat]
            normalized_timer = min(1.0, float(steps) / float(threshold))
            timers.append(normalized_timer)
        return np.array(timers, dtype=np.float32)
    
    def _compute_action_entropy(self):
        """
        æ”¹è¿›1: è®¡ç®—action entropyç”¨äºshaping
        åŸºäºæœ€è¿‘50æ­¥çš„actionåˆ†å¸ƒ
        """
        if len(self.action_history) == 0:
            return 0.0
        
        action_hist = np.zeros(12, dtype=np.float32)
        for act in self.action_history:
            action_hist[int(act)] += 1.0
        
        freq = action_hist / (action_hist.sum() + 1e-8)
        entropy = -np.sum(freq * np.log(freq + 1e-8))
        return float(entropy)
    
    def _compute_timer_sum(self):
        """
        æ”¹è¿›2: è®¡ç®—timeræ€»å’Œç”¨äºtimer-aware shaping
        """
        timer_obs = self._compute_timer_observation()
        return float(np.sum(timer_obs))

    def _global_success(self) -> bool:
        return bool(np.all(self.gaps < self.tolerance))

    def _apply_fixed_forgetting(self, mastery: np.ndarray) -> tuple[np.ndarray, dict]:
        """ä¿®å¤C: è¿ç»­é—å¿˜è¡°å‡çš„å›ºå®šé—å¿˜æœºåˆ¶"""
        # å¢å¼ºå‚æ•°ï¼ˆç»Ÿä¸€å¢å¼ºï¼‰
        enhanced_params = {
            'thresholds': {
                'algebra': 5,
                'geometry': 4,
                'statistics': 6
            },
            'strengths': {
                'algebra': 0.07,
                'geometry': 0.07,
                'statistics': 0.07
            },
            'adjust_factor': 0.6,
        }
        
        enhanced_params.update(self._fp.get('fixed', {}))
        events = {k: False for k in self.categories}
        
        for cat, idxs in self.categories.items():
            t = int(self.steps_since_review_cat[cat])
            th = int(enhanced_params['thresholds'][cat])
            
            # ä¿®å¤C: è¿ç»­é—å¿˜è¡°å‡ (é¿å…ç¡¬è·³å˜)
            if t >= th:
                s = float(enhanced_params['strengths'][cat])
                adj = float(enhanced_params.get('adjust_factor', 0.5))
                
                # è¿ç»­è¡°å‡: forgetting_strength = min(1.0, (timer - threshold) / scale)
                excess_steps = max(0, t - th)
                scale = 5.0  # æ§åˆ¶è¡°å‡å¹³æ»‘åº¦
                forgetting_strength = min(1.0, float(excess_steps) / scale)
                
                # åº”ç”¨è¿ç»­è¡°å‡
                effective_decay = 1.0 - (s * adj * forgetting_strength)
                
                for i in idxs:
                    mastery[i] = float(max(0.0, mastery[i] * effective_decay))
                
                events[cat] = True
                self.forgetting_trigger_stats[cat] += 1
                
        return mastery, events

    def _apply_improved_forgetting(self, mastery: np.ndarray) -> tuple[np.ndarray, dict]:
        """ä¿®å¤C: è¿ç»­é—å¿˜è¡°å‡çš„æ”¹è¿›é—å¿˜æœºåˆ¶"""
        # ä½¿ç”¨ä¸å¢å¼ºFFç›¸åŒçš„è§¦å‘é˜ˆå€¼ï¼ˆåŒæ­¥å¢å¼ºï¼Œä¿æŒä¸FFä¸€è‡´ï¼‰
        thresholds = {
            'algebra': 5,
            'geometry': 4,
            'statistics': 6
        }
        
        # IFç‰¹æœ‰å‚æ•° - Stability-Aware Threshold Forgetting
        if_params = {
            'base_decay': 0.07,
            'stability_k': 8.0,
            'stability_lambda': 0.7,
        }
        if_params.update(self._fp.get('improved', {}))
        
        ed = {k: 0.0 for k in self.categories}  # è®°å½•å®é™…è¡°å‡å¼ºåº¦
        
        for cat, idxs in self.categories.items():
            t_c = int(self.steps_since_review_cat[cat])
            th = int(thresholds[cat])
            
            # é˜ˆå€¼è§¦å‘ (ä¸FFå®Œå…¨ä¸€è‡´çš„è§¦å‘æ¡ä»¶)
            if t_c >= th:
                # è®¡ç®—å†å²ç¨³å®šæ€§
                hist = list(self.history_buffer_cat.get(cat, []))
                if len(hist) >= 2:
                    vol_c = float(np.std(hist))
                    stability_c = float(np.exp(-if_params['stability_k'] * vol_c))
                else:
                    stability_c = 0.5  # é»˜è®¤ä¸­ç­‰ç¨³å®šæ€§
                
                # Stability-Aware Threshold Forgetting æ ¸å¿ƒåˆ›æ–°
                base_decay = if_params['base_decay']
                lambda_factor = if_params['stability_lambda']
                
                # ä¿®å¤C: è¿ç»­è¡°å‡ + ç¨³å®šæ€§æ„ŸçŸ¥
                excess_steps = max(0, t_c - th)
                scale = 5.0  # æ§åˆ¶è¡°å‡å¹³æ»‘åº¦
                forgetting_strength = min(1.0, float(excess_steps) / scale)
                
                # IFçš„å…³é”®ä¸€åˆ€: effective_decay = base_decay * (1 - Î» * stability_c) * forgetting_strength
                effective_decay_base = base_decay * (1.0 - lambda_factor * stability_c)
                effective_decay = effective_decay_base * forgetting_strength
                
                # åº”ç”¨é—å¿˜ (ä½¿ç”¨adjust_factorä¿æŒä¸FFä¸€è‡´çš„åº”ç”¨æ–¹å¼)
                decay_multiplier = 1.0 - (effective_decay * 0.6)
                for i in idxs:
                    mastery[i] = float(max(0.0, mastery[i] * decay_multiplier))
                
                ed[cat] = effective_decay
                self.forgetting_trigger_stats[cat] += 1
                
        return mastery, ed

    def step(self, action: int):
        self.step_count += 1
        prev_sum = float(np.sum(self.gaps))
        prev_completed = int(np.sum(self.gaps < self.tolerance))
        
        skill_id = int(action)
        skill_gap = float(self.gaps[skill_id])
        
        # ç¡®ä¿å¿…é¡»æœ‰12ä¸ªä½å±‚ç­–ç•¥
        assert len(self.low_policies) == 12, "å¿…é¡»æä¾›12ä¸ªä½å±‚ç­–ç•¥"
        # ä¿®å¤A: æ‰©å±•ä½å±‚è§‚å¯Ÿç©ºé—´ [gap, resource_fatigue, mastery_category, action_counter]
        mastery = 1.0 - skill_gap
        mastery_category = 0.0 if mastery <= 0.33 else (0.5 if mastery <= 0.66 else 1.0)
        obs_skill = np.array([skill_gap, mastery_category], dtype=np.float32)
        low_policy = self.low_policies[skill_id]
        act, _ = low_policy.predict(obs_skill, deterministic=True)
        type_id = int(act)
        
        # è®¡ç®—æŒæ¡åº¦å’Œéš¾åº¦çº§åˆ« (ä¸MiniSkillEnvä¸€è‡´)
        mastery = 1.0 - skill_gap
        if mastery <= 0.33:
            level = "low"
        elif mastery <= 0.66:
            level = "medium"
        else:
            level = "high"
        
        # åŠ¨ä½œè¯­ä¹‰å®šä¹‰ (ä¸MiniSkillEnvä¸€è‡´)
        action_semantics = [
            "easy_res", "medium_res", "hard_res",
            "seek_teacher", "peer_discussion", "self_learn"
        ]
        
        # å¢ç›ŠçŸ©é˜µ (ä¸MiniSkillEnvå®Œå…¨ä¸€è‡´)
        GAIN_MATRIX = {
            "low": {
                "easy_res": 0.1, "medium_res": 0.05, "hard_res": 0.03,
                "seek_teacher": 0.05, "peer_discussion": 0.025, "self_learn": 0.015
            },
            "medium": {
                "easy_res": 0.05, "medium_res": 0.1, "hard_res": 0.05,
                "seek_teacher": 0.025, "peer_discussion": 0.05, "self_learn": 0.025
            },
            "high": {
                "easy_res": 0.03, "medium_res": 0.05, "hard_res": 0.1,
                "seek_teacher": 0.015, "peer_discussion": 0.025, "self_learn": 0.05
            }
        }
        
        # ä½¿ç”¨GAIN_MATRIXè®¡ç®—å¢ç›Š (ä¸MiniSkillEnvä¸€è‡´)
        action_name = action_semantics[type_id]
        gain = GAIN_MATRIX[level][action_name]
        
        # å™ªå£° (ä¸MiniSkillEnvä¸€è‡´)
        noise = float(self.np_random.normal(0.0, 0.001))
        
        # è®¡ç®—æ–°çš„gap (ä¸MiniSkillEnvä¸€è‡´)
        delta = 0.0 if (skill_gap < self.tolerance) else gain
        new_gap = clamp(skill_gap - delta + noise, 0.0, 1.0)
        self.gaps[skill_id] = new_gap
        
        # æ›´æ–°é—å¿˜æœºåˆ¶çŠ¶æ€
        for cat, idxs in self.categories.items():
            if skill_id in idxs:
                self.steps_since_review_cat[cat] = 0
            else:
                self.steps_since_review_cat[cat] += 1
                
        # åº”ç”¨é—å¿˜æœºåˆ¶
        mastery = 1.0 - self.gaps.copy()
        mastery_before = mastery.copy()
        forgetting_event = {'algebra': False, 'geometry': False, 'statistics': False}
        effective_decay_cat = {'algebra': 0.0, 'geometry': 0.0, 'statistics': 0.0}
        
        if self.forgetting_mode == 'fixed_forgetting':
            mastery, fe = self._apply_fixed_forgetting(mastery)
            forgetting_event.update(fe)
        elif self.forgetting_mode == 'improved_forgetting':
            mastery, ed = self._apply_improved_forgetting(mastery)
            effective_decay_cat.update(ed)
        
        # æ›´æ–°gaps
        self.gaps = np.clip(1.0 - mastery, 0.0, 1.0)
        
        # æ”¹è¿›1: æ›´æ–°action historyç”¨äºentropyè®¡ç®—
        self.action_history.append(skill_id)
        if len(self.action_history) > self.action_history_window:
            self.action_history = self.action_history[-self.action_history_window:]
        
        # ã€ä¿®å¤15ã€‘åŠ¨ä½œé‡å¤æƒ©ç½š
        repetition_penalty = 0.0
        if len(self.last_actions) >= 2:
            # å¦‚æœæœ€è¿‘é¢‘ç¹é€‰æ‹©åŒä¸€åŠ¨ä½œï¼Œç»™äºˆæƒ©ç½š
            recent_actions = self.last_actions[-5:] if len(self.last_actions) >= 5 else self.last_actions
            same_action_count = sum(1 for a in recent_actions if a == skill_id)
            if same_action_count >= 3:  # æœ€è¿‘5æ­¥å†…é€‰æ‹©åŒä¸€åŠ¨ä½œ3æ¬¡ä»¥ä¸Š
                repetition_penalty = -self.action_penalty_coef * same_action_count
        
        # æ›´æ–°åŠ¨ä½œè®°å¿†
        self.last_actions.append(skill_id)
        if len(self.last_actions) > self.action_memory_size:
            self.last_actions.pop(0)
        
        # è®¡ç®—å¥–åŠ±
        curr_sum = float(np.sum(self.gaps))
        curr_completed = int(np.sum(self.gaps < self.tolerance))
        delta_sum = prev_sum - curr_sum
        
        # åŸºç¡€å¥–åŠ±ç»“æ„
        reward = 0.0
        reward += 5.0 * delta_sum  # gapå‡å°‘å¥–åŠ±
        reward += 5.0 * max(0, curr_completed - prev_completed)
        reward += 20.0 if self._global_success() else 0.0  # å…¨å±€æˆåŠŸå¥–åŠ±
        
        # ã€ä¿®å¤15ã€‘æ·»åŠ åŠ¨ä½œé‡å¤æƒ©ç½š
        reward += repetition_penalty
        
        # ã€ä¿®å¤16ã€‘å‡è¡¡æ€§å¥–åŠ±ï¼ˆé¼“åŠ±è¦†ç›–æ‰€æœ‰æŠ€èƒ½ï¼‰
        completed_skills = np.sum(self.gaps < self.tolerance)
        balance_reward = 0.1 * (completed_skills / 12.0)  # å®Œæˆè¶Šå¤šå¥–åŠ±è¶Šé«˜
        reward += balance_reward
        
        # ã€ä¿®å¤18ã€‘å¢åŠ ç†µå¥–åŠ±ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰
        current_entropy = self._compute_action_entropy()
        entropy_bonus = 0.03 * current_entropy  # å°å¹…ç†µå¥–åŠ±
        reward += entropy_bonus
        
        reward += -0.02  # æ­¥æƒ©ç½š
        
        # ã€EXP15ä¿®å¤ã€‘å¼ºåˆ¶ç­–ç•¥å…³æ³¨æ‰€æœ‰ç»´åº¦ - ç»´åº¦å‡è¡¡æ€§æƒ©ç½š
        gap_variance = np.var(self.gaps)
        if gap_variance > 0.05:  # é˜ˆå€¼å¯è°ƒï¼Œæ–¹å·®å¤§åˆ™æƒ©ç½šé‡
            reward -= 5.0 * gap_variance  # æƒ©ç½šä¸å‡è¡¡çš„å­¦ä¹ 
        else:
            reward += 0.5  # å¥–åŠ±å‡è¡¡çš„å­¦ä¹ 
        
        # ã€EXP15ä¿®å¤ã€‘Timeræƒ©ç½š - æé«˜æƒ©ç½šç³»æ•°ï¼Œè¿«ä½¿ç­–ç•¥å¿…é¡»è½®æ¢
        timer_penalty_coef = 0.0  # é»˜è®¤æ— æƒ©ç½š
        if self.forgetting_mode in ['fixed_forgetting', 'improved_forgetting']:
            timer_penalty_coef = 2.0
        
        timer_sum = self._compute_timer_sum()
        reward -= timer_penalty_coef * timer_sum  # æ–°å¢Timeræƒ©ç½š
        
        # æ”¹è¿›1: å¼±action-entropy shaping (æ‰€æœ‰Managerè®­ç»ƒ)
        entropy = self._compute_action_entropy()
        reward += self.entropy_coef * entropy
        
        terminated = self._global_success()
        truncated = bool(self.step_count >= self.max_steps)
        self._last_action = skill_id
        
        # åŠ¨ä½œæ©ç ï¼š12ä¸ªåŠ¨ä½œ
        mask = (self.gaps >= self.tolerance).astype(np.int32)
        
        # ä¿®å¤B: æ‰©å±•è§‚å¯Ÿç©ºé—´åŒ…å«å½’ä¸€åŒ–timer
        timer_obs = self._compute_timer_observation()
        obs = np.concatenate([self.gaps, timer_obs])
        
        # æ”¹è¿›6: è¯Šæ–­æ—¥å¿—
        gap_variance = float(np.var(self.gaps))
        action_freq = np.zeros(12, dtype=np.float32)
        for act in self.action_history:
            action_freq[int(act)] += 1.0
        action_freq = action_freq / (action_freq.sum() + 1e-8)
        
        info = {
            # åŸºç¡€æŒ‡æ ‡
            'sum_prev': prev_sum,
            'sum_curr': curr_sum,
            'skill_id': skill_id,
            'low_action': int(type_id),
            'delta': delta,
            'noise': noise,
            'action_mask': mask.tolist(),
            'forgetting_mode': self.forgetting_mode,
            
            # å…³é”®è¯Šæ–­æŒ‡æ ‡
            'gap_variance': gap_variance,  # å­¦ä¹ å‡è¡¡æ€§
            'action_entropy': entropy,  # ç­–ç•¥æ¢ç´¢æ°´å¹³
            'timer_mean': float(np.mean(timer_obs)),  # timerçŠ¶æ€
            'timer_std': float(np.std(timer_obs)),  # timeræ³¢åŠ¨æ€§
            
            # é—å¿˜ç›¸å…³
            'forgetting_triggers': dict(self.forgetting_trigger_stats),  # å„ç±»åˆ«è§¦å‘æ¬¡æ•°
            'effective_decay_cat': effective_decay_cat,  # å®é™…è¡°å‡å¼ºåº¦
            'steps_since_review_cat': dict(self.steps_since_review_cat),  # è·ç¦»ä¸Šæ¬¡å¤ä¹ 
            'forgetting_event': forgetting_event,  # æœ¬æ¬¡æ˜¯å¦è§¦å‘
            
            # ç­–ç•¥åˆ†æ
            'action_freq': action_freq.tolist(),  # åŠ¨ä½œä½¿ç”¨é¢‘ç‡
            'gap_mean': float(np.mean(self.gaps)),  # å¹³å‡gap
            
            # ä¿ç•™åŸæœ‰å­—æ®µï¼ˆå‘åå…¼å®¹ï¼‰
            'timer_obs': timer_obs.tolist(),
            'mastery_before': mastery_before.tolist(),
            'mastery_after': mastery.tolist(),
            'timer_sum': timer_sum,
        }

        # ã€EXP15ä¿®å¤ã€‘å¼ºåˆ¶ç­–ç•¥å…³æ³¨æ‰€æœ‰ç»´åº¦ - é˜²æ­¢åç¼©
        gap_variance = float(np.var(self.gaps))
        if gap_variance > 0.05:
            reward -= 5.0 * gap_variance  # æƒ©ç½šä¸å‡è¡¡çš„å­¦ä¹ 
        else:
            reward += 0.5  # å¥–åŠ±å‡è¡¡çš„å­¦ä¹ 
        
        # æ›´æ–°å†å²ç¼“å†²åŒº
        for cat, idxs in self.categories.items():
            M_c = float(np.mean(mastery[idxs]))
            hb = self.history_buffer_cat.get(cat, [])
            hb = hb + [M_c]
            if len(hb) > 50:
                hb = hb[-50:]
            self.history_buffer_cat[cat] = hb
            
        return obs, reward, terminated, truncated, info


class FlatMiniEnv12(gym.Env):
    """
    12ç»´ Flat PPO ç¯å¢ƒï¼ˆä¿®å¤ç­–ç•¥åç¼©é—®é¢˜ï¼‰
    
    ä¿®å¤è¯´æ˜ï¼š
    - æ‰©å±•è§‚å¯Ÿç©ºé—´ï¼š[gap_0, ..., gap_11, timer_cat0, timer_cat1, timer_cat2] è§£å†³POMDPé—®é¢˜
    - è¿ç»­é—å¿˜è¡°å‡ï¼šé¿å…ç¡¬è·³å˜å¯¼è‡´çš„ç­–ç•¥åç¼©
    - ä¿ç•™æ‰€æœ‰åŸæœ‰æœºåˆ¶ï¼šé—å¿˜ã€èµ„æºçæƒœã€éš¾åº¦åŒ¹é…ç­‰
    """

    def __init__(self, tolerance: float = 0.05, max_steps: int = 200, seed: int = 0, 
                 forgetting_mode: str = 'no_forgetting', forgetting_params: dict | None = None,
                 resource_enabled: bool = False, resource_decay_range: tuple | list = (0.2, 0.4), 
                 match_bonus: float = 0.2, mismatch_penalty: float = 0.1, 
                 timing_bonus: float = 0.03, timing_penalty: float = 0.03, 
                 difficulty_bins: tuple | list = (0.33, 0.66)):
        super().__init__()
        self.tolerance = float(tolerance)
        self.max_steps = int(max_steps)
        self.np_random, _ = gym.utils.seeding.np_random(seed)
        
        # ä¿®å¤B: æ‰©å±•è§‚å¯Ÿç©ºé—´ [gap_0, ..., gap_11, timer_cat0, timer_cat1, timer_cat2]
        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(15,), dtype=np.float32)
        self.action_space = spaces.Discrete(72)  # 12 skills Ã— 6 actions
        
        # ğŸ”§ ä¿®å¤ä¸å…¬å¹³ç‚¹1: ä½¿ç”¨ä¸åˆ†å±‚ç¯å¢ƒç›¸åŒçš„GAIN_MATRIX
        # åŠ¨ä½œè¯­ä¹‰å®šä¹‰ (ä¸MiniSkillEnvä¿æŒä¸€è‡´)
        self.action_semantics = [
            "easy_res", "medium_res", "hard_res",
            "seek_teacher", "peer_discussion", "self_learn"
        ]
        
        # å¢ç›ŠçŸ©é˜µï¼ˆä¸MiniSkillEnvå®Œå…¨ä¸€è‡´çš„ç¯å¢ƒåŠ¨åŠ›å­¦ï¼‰
        self.GAIN_MATRIX = {
            "low": {        # æŒæ¡åº¦ 0.00-0.33
                "easy_res": 0.1, "medium_res": 0.05, "hard_res": 0.03,
                "seek_teacher": 0.05, "peer_discussion": 0.025, "self_learn": 0.015
            },
            "medium": {     # æŒæ¡åº¦ 0.34-0.66
                "easy_res": 0.05, "medium_res": 0.1, "hard_res": 0.05,
                "seek_teacher": 0.025, "peer_discussion": 0.05, "self_learn": 0.025
            },
            "high": {       # æŒæ¡åº¦ 0.67-1.00
                "easy_res": 0.03, "medium_res": 0.05, "hard_res": 0.1,
                "seek_teacher": 0.015, "peer_discussion": 0.025, "self_learn": 0.05
            }
        }
        
        # ä¿ç•™åŸæœ‰delta_mapä½œä¸ºå¤‡ç”¨ï¼ˆå‘åå…¼å®¹ï¼‰
        self.delta_map = np.array([0.05, 0.10, 0.20, 0.03, 0.06, 0.02], dtype=np.float32)
        
        # ğŸ”§ ä¿®å¤ä¸å…¬å¹³ç‚¹1: æ·»åŠ èµ„æºç–²åŠ³æœºåˆ¶ (ä¸MiniSkillEnvä¿æŒä¸€è‡´)
        self.resource_usage_count = np.zeros((12, 3), dtype=np.float32)  # 12ä¸ªæŠ€èƒ½ Ã— 3ä¸ªèµ„æºåŠ¨ä½œ
        self.resource_fatigue_alpha = 0.5  # ä¸MiniSkillEnvä¸€è‡´
        
        self.gaps = None
        self.step_count = 0
        self._last_action = None
        
        # é—å¿˜æœºåˆ¶æ”¯æŒ
        self.forgetting_mode = str(forgetting_mode)
        self.categories = {
            'algebra': [0, 1, 2, 3],
            'geometry': [4, 5, 6, 7],
            'statistics': [8, 9, 10, 11],
        }
        self.steps_since_review_cat = {'algebra': 0, 'geometry': 0, 'statistics': 0}
        self.history_buffer_cat = {'algebra': [], 'geometry': [], 'statistics': []}
        self._fp = forgetting_params or {}
        
        # è§¦å‘é¢‘ç‡ç»Ÿè®¡
        self.forgetting_trigger_stats = {'algebra': 0, 'geometry': 0, 'statistics': 0}
        
        # ä¿®å¤B: é—å¿˜é˜ˆå€¼ (ç”¨äºè®¡ç®—å½’ä¸€åŒ–timer)
        self.forgetting_thresholds = {
            'algebra': 5,
            'geometry': 4,
            'statistics': 6
        }
        
        # èµ„æºæœºåˆ¶æ”¯æŒ
        self.resource_enabled = bool(resource_enabled)
        self.resource_decay_range = tuple(resource_decay_range)
        self.match_bonus = float(match_bonus)
        self.mismatch_penalty = float(mismatch_penalty)
        self.timing_bonus = float(timing_bonus)
        self.timing_penalty = float(timing_penalty)
        self.difficulty_bins = tuple(difficulty_bins)
        self.resource_used = None
        
        # æ”¹è¿›1: å¼±action-entropy shaping - ç»´æŠ¤æ»‘åŠ¨çª—å£ (Flatä¹Ÿä½¿ç”¨)
        self.action_history = []
        self.action_history_window = 50
        # ğŸ”§ ä¿®å¤ä¸å…¬å¹³ç‚¹3: è°ƒæ•´entropyç³»æ•°ä»¥åŒ¹é…åŠ¨ä½œç©ºé—´ç»´åº¦
        # åˆ†å±‚æ˜¯12ç»´ï¼Œæ‰å¹³æ˜¯72ç»´ï¼Œæ‰€ä»¥æ‰å¹³çš„ç³»æ•°åº”è¯¥æ˜¯ 0.01 * 72/12 = 0.06
        self.entropy_coef = 0.06  # è°ƒæ•´ä¸º6å€ï¼Œè¡¥å¿72ç»´åŠ¨ä½œç©ºé—´çš„ç¨€é‡Šæ•ˆåº”

    def reset(self, *, seed: int | None = None, options: dict | None = None):
        if seed is not None:
            self.np_random, _ = gym.utils.seeding.np_random(seed)
        
        # åˆå§‹åŒ–12ç»´gaps
        if options and isinstance(options, dict) and 'init_gaps' in options:
            init = np.array(options['init_gaps'], dtype=np.float32)
            if init.shape != (12,):
                init = init.reshape(12,)
            self.gaps = np.clip(init, 0.0, 1.0)
        else:
            self.gaps = self.np_random.uniform(0.4, 0.9, size=(12,)).astype(np.float32)
        
        self.step_count = 0
        self._last_action = None
        
        # é‡ç½®é—å¿˜æœºåˆ¶çŠ¶æ€
        for c in self.steps_since_review_cat:
            self.steps_since_review_cat[c] = 0
        for c in self.forgetting_trigger_stats:
            self.forgetting_trigger_stats[c] = 0
        for c in self.history_buffer_cat:
            self.history_buffer_cat[c] = []
        
        # ğŸ”§ ä¿®å¤ä¸å…¬å¹³ç‚¹1: é‡ç½®èµ„æºç–²åŠ³åº¦
        self.resource_usage_count.fill(0.0)
        
        # æ”¹è¿›1: é‡ç½®action history
        self.action_history = []
        
        # ä¿®å¤B: æ‰©å±•è§‚å¯Ÿç©ºé—´åŒ…å«å½’ä¸€åŒ–timer
        timer_obs = self._compute_timer_observation()
        obs = np.concatenate([self.gaps, timer_obs])
        return obs, {}

    def _compute_timer_observation(self):
        """
        ä¿®å¤B: è®¡ç®—å½’ä¸€åŒ–timerè§‚å¯Ÿ (è§£å†³POMDPé—®é¢˜)
        """
        timers = []
        for cat in ['algebra', 'geometry', 'statistics']:
            steps = self.steps_since_review_cat[cat]
            threshold = self.forgetting_thresholds[cat]
            normalized_timer = min(1.0, float(steps) / float(threshold))
            timers.append(normalized_timer)
        return np.array(timers, dtype=np.float32)
    
    def _compute_action_entropy(self):
        """
        æ”¹è¿›1: è®¡ç®—action entropyç”¨äºshaping
        åŸºäºæœ€è¿‘50æ­¥çš„actionåˆ†å¸ƒ (Flatä½¿ç”¨72ä¸ªåŠ¨ä½œ)
        """
        if len(self.action_history) == 0:
            return 0.0
        
        action_hist = np.zeros(72, dtype=np.float32)
        for act in self.action_history:
            action_hist[int(act)] += 1.0
        
        freq = action_hist / (action_hist.sum() + 1e-8)
        entropy = -np.sum(freq * np.log(freq + 1e-8))
        return float(entropy)
    
    def _compute_timer_sum(self):
        timer_obs = self._compute_timer_observation()
        return float(np.sum(timer_obs))

    def _global_success(self) -> bool:
        return bool(np.all(self.gaps < self.tolerance))

    def _apply_fixed_forgetting(self, mastery: np.ndarray) -> tuple[np.ndarray, dict]:
        """ä¿®å¤C: è¿ç»­é—å¿˜è¡°å‡çš„å›ºå®šé—å¿˜æœºåˆ¶"""
        enhanced_params = {
            'thresholds': {
                'algebra': 5, 'geometry': 4, 'statistics': 6
            },
            'strengths': {
                'algebra': 0.07, 'geometry': 0.07, 'statistics': 0.07
            },
            'adjust_factor': 0.6,
        }
        
        enhanced_params.update(self._fp.get('fixed', {}))
        events = {k: False for k in self.categories}
        
        for cat, idxs in self.categories.items():
            t = int(self.steps_since_review_cat[cat])
            th = int(enhanced_params['thresholds'][cat])
            
            if t >= th:
                s = float(enhanced_params['strengths'][cat])
                adj = float(enhanced_params.get('adjust_factor', 0.5))
                
                # ä¿®å¤C: è¿ç»­è¡°å‡
                excess_steps = max(0, t - th)
                scale = 5.0
                forgetting_strength = min(1.0, float(excess_steps) / scale)
                effective_decay = 1.0 - (s * adj * forgetting_strength)
                
                for i in idxs:
                    mastery[i] = float(max(0.0, mastery[i] * effective_decay))
                
                events[cat] = True
                self.forgetting_trigger_stats[cat] += 1
                
        return mastery, events

    def _apply_improved_forgetting(self, mastery: np.ndarray) -> tuple[np.ndarray, dict]:
        """ä¿®å¤C: è¿ç»­é—å¿˜è¡°å‡çš„æ”¹è¿›é—å¿˜æœºåˆ¶"""
        thresholds = {'algebra': 5, 'geometry': 4, 'statistics': 6}
        
        if_params = {
            'base_decay': 0.07,
            'stability_k': 8.0,
            'stability_lambda': 0.7,
        }
        if_params.update(self._fp.get('improved', {}))
        
        ed = {k: 0.0 for k in self.categories}
        
        for cat, idxs in self.categories.items():
            t_c = int(self.steps_since_review_cat[cat])
            th = int(thresholds[cat])
            
            if t_c >= th:
                hist = list(self.history_buffer_cat.get(cat, []))
                if len(hist) >= 2:
                    vol_c = float(np.std(hist))
                    stability_c = float(np.exp(-if_params['stability_k'] * vol_c))
                else:
                    stability_c = 0.5
                
                base_decay = if_params['base_decay']
                lambda_factor = if_params['stability_lambda']
                
                # ä¿®å¤C: è¿ç»­è¡°å‡ + ç¨³å®šæ€§æ„ŸçŸ¥
                excess_steps = max(0, t_c - th)
                scale = 5.0
                forgetting_strength = min(1.0, float(excess_steps) / scale)
                
                effective_decay_base = base_decay * (1.0 - lambda_factor * stability_c)
                effective_decay = effective_decay_base * forgetting_strength
                
                decay_multiplier = 1.0 - (effective_decay * 0.6)
                for i in idxs:
                    mastery[i] = float(max(0.0, mastery[i] * decay_multiplier))
                
                ed[cat] = effective_decay
                self.forgetting_trigger_stats[cat] += 1
                
        return mastery, ed

    def step(self, action: int):
        self.step_count += 1
        prev_sum = float(np.sum(self.gaps))
        prev_completed = int(np.sum(self.gaps < self.tolerance))

        # è§£ç åŠ¨ä½œï¼š72ä¸ªåŠ¨ä½œ = 12ä¸ªæŠ€èƒ½ Ã— 6ä¸ªåŠ¨ä½œç±»å‹
        skill_id = int(action // 6)
        type_id = int(action % 6)
        
        # ğŸ”§ ä¿®å¤ä¸å…¬å¹³ç‚¹1: ä½¿ç”¨ä¸åˆ†å±‚ç¯å¢ƒç›¸åŒçš„difficulty-awareåŠ¨åŠ›å­¦
        skill_gap = float(self.gaps[skill_id])
        mastery = 1.0 - skill_gap
        
        # ç¡®å®šæŒæ¡åº¦åŒºé—´ (ä¸MiniSkillEnvå®Œå…¨ä¸€è‡´)
        if mastery <= 0.33:
            level = "low"
        elif mastery <= 0.66:
            level = "medium"
        else:
            level = "high"
        
        action_name = self.action_semantics[type_id]
        
        # ğŸ”§ ä¿®å¤ä¸å…¬å¹³ç‚¹1: æ·»åŠ èµ„æºè¡°å‡æœºåˆ¶ (ä¸MiniSkillEnvä¿æŒä¸€è‡´)
        resource_decay = 1.0
        if type_id in [0, 1, 2]:  # èµ„æºåŠ¨ä½œ
            res_id = type_id
            # æ›´æ–°ä½¿ç”¨æ¬¡æ•°
            self.resource_usage_count[skill_id, res_id] += 1.0
            # è¿ç»­è¡°å‡: resource_decay = exp(-alpha * usage_count)
            usage_count = self.resource_usage_count[skill_id, res_id]
            resource_decay = np.exp(-self.resource_fatigue_alpha * usage_count)
        
        # ä½¿ç”¨GAIN_MATRIXè®¡ç®—å¢ç›Š (ä¸åˆ†å±‚ç¯å¢ƒå®Œå…¨ä¸€è‡´çš„åŠ¨åŠ›å­¦)
        base_gain = self.GAIN_MATRIX[level][action_name]
        # åº”ç”¨èµ„æºè¡°å‡
        gain = base_gain * resource_decay
        
        # ğŸ”§ ä¿®å¤ä¸å…¬å¹³ç‚¹2: ä½¿ç”¨ä¸åˆ†å±‚ç¯å¢ƒç›¸åŒçš„å™ªå£°å¼ºåº¦
        noise = float(self.np_random.normal(0.0, 0.001))  # ä»0.005æ”¹ä¸º0.001
        
        # è®¡ç®—gapå˜åŒ– (ä½¿ç”¨difficulty-aware gain + èµ„æºè¡°å‡)
        delta = 0.0 if (skill_gap < self.tolerance) else gain
        new_gap = clamp(skill_gap - delta + noise, 0.0, 1.0)
        self.gaps[skill_id] = new_gap
        
        # æ›´æ–°é—å¿˜æœºåˆ¶çŠ¶æ€
        for cat, idxs in self.categories.items():
            if skill_id in idxs:
                self.steps_since_review_cat[cat] = 0
            else:
                self.steps_since_review_cat[cat] += 1

        # åº”ç”¨é—å¿˜æœºåˆ¶
        mastery = 1.0 - self.gaps.copy()
        mastery_before = mastery.copy()
        forgetting_event = {'algebra': False, 'geometry': False, 'statistics': False}
        effective_decay_cat = {'algebra': 0.0, 'geometry': 0.0, 'statistics': 0.0}
        
        if self.forgetting_mode == 'fixed_forgetting':
            mastery, fe = self._apply_fixed_forgetting(mastery)
            forgetting_event.update(fe)
        elif self.forgetting_mode == 'improved_forgetting':
            mastery, ed = self._apply_improved_forgetting(mastery)
            effective_decay_cat.update(ed)
        
        # æ›´æ–°gaps
        self.gaps = np.clip(1.0 - mastery, 0.0, 1.0)
        
        # æ”¹è¿›1: æ›´æ–°action historyç”¨äºentropyè®¡ç®—
        self.action_history.append(action)
        if len(self.action_history) > self.action_history_window:
            self.action_history = self.action_history[-self.action_history_window:]
        
        # è®¡ç®—å¥–åŠ±
        curr_sum = float(np.sum(self.gaps))
        curr_completed = int(np.sum(self.gaps < self.tolerance))
        delta_sum = prev_sum - curr_sum
        
        reward = 0.0
        reward += 5.0 * delta_sum
        reward += 5.0 * max(0, curr_completed - prev_completed)
        reward += 20.0 if self._global_success() else 0.0
        reward += -0.02
        
        # æ”¹è¿›1: å¼±action-entropy shaping (Flatä¹Ÿä½¿ç”¨ï¼Œä½†ä¸ä½¿ç”¨timer-aware shaping)
        entropy = self._compute_action_entropy()
        reward += self.entropy_coef * entropy

        
        # ã€EXP15ä¿®å¤ã€‘Timeræƒ©ç½š - æé«˜æƒ©ç½šç³»æ•°ï¼Œè¿«ä½¿ç­–ç•¥å¿…é¡»è½®æ¢
        timer_penalty_coef = 0.0  # é»˜è®¤æ— æƒ©ç½š
        if self.forgetting_mode in ['fixed_forgetting', 'improved_forgetting']:
            timer_penalty_coef = 2.0
        
        timer_sum = self._compute_timer_sum()
        reward -= timer_penalty_coef * timer_sum  # æ–°å¢Timeræƒ©ç½š

        terminated = self._global_success()
        truncated = bool(self.step_count >= self.max_steps)
        self._last_action = skill_id
        
        # åŠ¨ä½œæ©ç ï¼š72ä¸ªåŠ¨ä½œ
        mask = np.ones((72,), dtype=np.int32)
        for s in range(12):
            if self.gaps[s] < self.tolerance:
                mask[s*6:(s+1)*6] = 0
        
        # ä¿®å¤B: æ‰©å±•è§‚å¯Ÿç©ºé—´åŒ…å«å½’ä¸€åŒ–timer
        timer_obs = self._compute_timer_observation()
        obs = np.concatenate([self.gaps, timer_obs])
        
        # æ”¹è¿›6: è¯Šæ–­æ—¥å¿—
        gap_variance = float(np.var(self.gaps))
        action_freq = np.zeros(72, dtype=np.float32)
        for act in self.action_history:
            action_freq[int(act)] += 1.0
        action_freq = action_freq / (action_freq.sum() + 1e-8)
        
        info = {
            # åŸºç¡€æŒ‡æ ‡
            "sum_prev": prev_sum,
            "sum_curr": curr_sum,
            "skill_id": skill_id,
            "type_id": type_id,
            "delta": delta,
            "noise": noise,
            "action_mask": mask.tolist(),
            "forgetting_mode": self.forgetting_mode,
            
            # å…³é”®è¯Šæ–­æŒ‡æ ‡ï¼ˆä¸åˆ†å±‚ç¯å¢ƒå®Œå…¨ä¸€è‡´ï¼‰
            'gap_variance': gap_variance,  # å­¦ä¹ å‡è¡¡æ€§
            'action_entropy': entropy,  # ç­–ç•¥æ¢ç´¢æ°´å¹³
            'timer_mean': float(np.mean(timer_obs)),  # timerçŠ¶æ€
            'timer_std': float(np.std(timer_obs)),  # timeræ³¢åŠ¨æ€§
            
            # é—å¿˜ç›¸å…³ï¼ˆä¸åˆ†å±‚ç¯å¢ƒå®Œå…¨ä¸€è‡´ï¼‰
            'forgetting_triggers': dict(self.forgetting_trigger_stats),  # å„ç±»åˆ«è§¦å‘æ¬¡æ•°
            'effective_decay_cat': effective_decay_cat,  # å®é™…è¡°å‡å¼ºåº¦
            'steps_since_review_cat': dict(self.steps_since_review_cat),  # è·ç¦»ä¸Šæ¬¡å¤ä¹ 
            'forgetting_event': forgetting_event,  # æœ¬æ¬¡æ˜¯å¦è§¦å‘
            
            # ç­–ç•¥åˆ†æ
            'action_freq': action_freq.tolist(),  # åŠ¨ä½œä½¿ç”¨é¢‘ç‡
            'gap_mean': float(np.mean(self.gaps)),  # å¹³å‡gap
            
            # ä¿ç•™åŸæœ‰å­—æ®µï¼ˆå‘åå…¼å®¹ï¼‰
            "timer_obs": timer_obs.tolist(),
            "mastery_before": mastery_before.tolist(),
            "mastery_after": mastery.tolist(),
        }

        # ã€EXP15ä¿®å¤ã€‘å¼ºåˆ¶ç­–ç•¥å…³æ³¨æ‰€æœ‰ç»´åº¦ - é˜²æ­¢åç¼©
        gap_variance = float(np.var(self.gaps))
        if gap_variance > 0.05:
            reward -= 5.0 * gap_variance  # æƒ©ç½šä¸å‡è¡¡çš„å­¦ä¹ 
        else:
            reward += 0.5  # å¥–åŠ±å‡è¡¡çš„å­¦ä¹ 
        
        # æ›´æ–°å†å²ç¼“å†²åŒº
        for cat, idxs in self.categories.items():
            M_c = float(np.mean(mastery[idxs]))
            hb = self.history_buffer_cat.get(cat, [])
            hb = hb + [M_c]
            if len(hb) > 50:
                hb = hb[-50:]
            self.history_buffer_cat[cat] = hb
            
        return obs, reward, terminated, truncated, info
