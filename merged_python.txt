============================================================
Pythoné¡¹ç›®æºç åˆå¹¶æ–‡ä»¶
============================================================

é¡¹ç›®ç»“æ„æ ‘çŠ¶å›¾:
----------------------------------------
mini/
â”œâ”€â”€ env/
â”‚   â””â”€â”€ v6_mini_env.py
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ manager_bc_warmup.py
â””â”€â”€ train/
    â”œâ”€â”€ train_low_level.py
    â””â”€â”€ train_manager.py

============================================================

æ–‡ä»¶: env\v6_mini_env.py
----------------------------------------
# -*- coding: utf-8 -*-#
"""
ä¿®å¤åçš„åˆ†å±‚å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ
è§£å†³ç­–ç•¥åç¼©é—®é¢˜çš„å…³é”®ä¿®å¤ï¼š
1. ä½å±‚ï¼šè¿ç»­èµ„æºè¡°å‡ + æ‰©å±•è§‚å¯Ÿç©ºé—´ [gap, resource_fatigue]
2. é«˜å±‚ï¼šæ‰©å±•è§‚å¯Ÿç©ºé—´ [gaps, timers] + è¿ç»­é—å¿˜è¡°å‡
3. ä¿ç•™æ‰€æœ‰åŸæœ‰æœºåˆ¶ï¼šé—å¿˜ã€èµ„æºçæƒœã€éš¾åº¦åŒ¹é…
"""
import numpy as np
import gymnasium as gym
from gymnasium import spaces


def clamp(x: float, lo: float, hi: float) -> float:
    return float(max(lo, min(hi, x)))


class MiniSkillEnv(gym.Env):
    """
    ä½å±‚æŠ€èƒ½ç¯å¢ƒï¼ˆä¿®å¤ç­–ç•¥åç¼©é—®é¢˜ï¼‰
    
    ä¿®å¤è¯´æ˜ï¼š
    - æ‰©å±•è§‚å¯Ÿç©ºé—´ï¼š[gap, resource_fatigue, mastery_category, action_counter] è§£å†³èµ„æºçŠ¶æ€ä¸å¯è§‚æµ‹é—®é¢˜
    - è¿ç»­èµ„æºè¡°å‡ï¼šé¿å…ç¡¬è·³å˜å¯¼è‡´çš„ç­–ç•¥åç¼©
    - ä¿ç•™æ‰€æœ‰åŸæœ‰æœºåˆ¶ï¼šéš¾åº¦åŒ¹é…ã€èµ„æºçæƒœç­‰
    """

    metadata = {"render.modes": ["human"]}

    def __init__(self, tolerance: float = 0.05, max_steps: int = 200, seed: int = 0):
        super().__init__()
        self.tolerance = float(tolerance)
        self.max_steps = int(max_steps)
        self.np_random, _ = gym.utils.seeding.np_random(seed)

        # ã€ä¿®å¤1ã€‘å¢å¼ºè§‚å¯Ÿç©ºé—´ï¼ˆå­¦æœ¯ç†ç”±ï¼šéƒ¨åˆ†å¯è§‚æµ‹â†’å…¨å¯è§‚æµ‹ï¼‰
        # åŸ2ç»´ï¼š[gap, resource_fatigue]
        # æ–°4ç»´ï¼š[gap, resource_fatigue, mastery_category, action_counter]
        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(4,), dtype=np.float32)
        self.action_space = spaces.Discrete(6)

        # åŠ¨ä½œè¯­ä¹‰å®šä¹‰
        self.action_semantics = [
            "easy_res", "medium_res", "hard_res",
            "seek_teacher", "peer_discussion", "self_learn"
        ]

        # å¢ç›ŠçŸ©é˜µï¼ˆç¯å¢ƒåŠ¨åŠ›å­¦ï¼Œä¸æ˜¯å¥–åŠ±ï¼‰
        self.GAIN_MATRIX = {
            "low": {        # æŒæ¡åº¦ 0.00-0.33
                "easy_res": 0.1, "medium_res": 0.05, "hard_res": 0.03,
                "seek_teacher": 0.05, "peer_discussion": 0.025, "self_learn": 0.015
            },
            "medium": {     # æŒæ¡åº¦ 0.34-0.66
                "easy_res": 0.05, "medium_res": 0.1, "hard_res": 0.05,
                "seek_teacher": 0.025, "peer_discussion": 0.05, "self_learn": 0.025
            },
            "high": {       # æŒæ¡åº¦ 0.67-1.00
                "easy_res": 0.03, "medium_res": 0.05, "hard_res": 0.1,
                "seek_teacher": 0.015, "peer_discussion": 0.025, "self_learn": 0.05
            }
        }

        # ä¿ç•™åŸæœ‰delta_mapä½œä¸ºå¤‡ç”¨ï¼ˆå‘åå…¼å®¹ï¼‰
        self.delta_map = np.array([0.05, 0.10, 0.20, 0.03, 0.06, 0.02], dtype=np.float32)

        # ã€ä¿®å¤2ã€‘å¢åŠ åŠ¨ä½œè®¡æ•°å™¨ï¼ˆé˜²æ­¢é‡å¤é€‰æ‹©åŒä¸€åŠ¨ä½œï¼‰
        self.action_counter = np.zeros(6, dtype=np.float32)
        self.action_decay = 0.9  # æ¯æ­¥è¡°å‡ç³»æ•°

        # ä¿®å¤A: èµ„æºç–²åŠ³åº¦è·Ÿè¸ª (è¿ç»­è¡°å‡æ›¿ä»£ç¡¬è·³å˜)
        self.resource_usage_count = np.zeros(3, dtype=np.float32)  # 3ä¸ªèµ„æºåŠ¨ä½œçš„ä½¿ç”¨æ¬¡æ•°
        # ã€ä¿®å¤3ã€‘è°ƒæ•´èµ„æºç–²åŠ³å‚æ•°
        self.resource_fatigue_alpha = 0.7  # ä»0.5æé«˜åˆ°0.7ï¼Œè¡°å‡æ›´å¿«

        self.gap = None
        self.step_count = 0

    def reset(self, *, seed: int | None = None, options: dict | None = None):
        if seed is not None:
            self.np_random, _ = gym.utils.seeding.np_random(seed)
        
        # 1. éšæœº Gapï¼šè¦†ç›–å…¨éš¾åº¦
        self.gap = float(self.np_random.uniform(0.1, 0.95))
        
        # ã€ä¿®å¤4ã€‘é‡ç½®æ—¶ç»™äºˆé€‚ä¸­çš„èµ„æºç–²åŠ³åº¦ï¼ˆä¸æ˜¯ä»0å¼€å§‹ï¼‰
        self.resource_usage_count = self.np_random.uniform(1.0, 4.0, size=(3,)).astype(np.float32)
        self.action_counter.fill(0.0)
        
        self.step_count = 0
        # ã€å…³é”®ã€‘æ‰©å±•è§‚å¯Ÿç©ºé—´åŒ…å«èµ„æºç–²åŠ³åº¦
        resource_fatigue = self._compute_resource_fatigue()
        mastery = 1.0 - self.gap
        mastery_category = 0.0 if mastery <= 0.33 else (0.5 if mastery <= 0.66 else 1.0)
        
        obs = np.array([
            self.gap, 
            resource_fatigue,
            mastery_category,
            0.0  # åˆå§‹åŠ¨ä½œè®¡æ•°å™¨
        ], dtype=np.float32)
        info = {}
        return obs, info

    def _compute_resource_fatigue(self):
        """
        ä¿®å¤A: è®¡ç®—èµ„æºç–²åŠ³åº¦ (è¿ç»­å€¼ï¼Œé¿å…ç¡¬è·³å˜)
        ä½¿ç”¨æŒ‡æ•°è¡°å‡: fatigue = 1 - exp(-alpha * max_usage_count)
        """
        max_usage = np.max(self.resource_usage_count)
        fatigue = 1.0 - np.exp(-self.resource_fatigue_alpha * max_usage)
        return float(np.clip(fatigue, 0.0, 1.0))

    def step(self, action: int):
        self.step_count += 1

        # ã€ä¿®å¤5ã€‘æ›´æ–°åŠ¨ä½œè®¡æ•°å™¨
        self.action_counter = self.action_counter * self.action_decay
        self.action_counter[action] += 1.0

        prev_gap = float(self.gap)
        mastery = 1.0 - prev_gap

        # ç¡®å®šæŒæ¡åº¦åŒºé—´
        if mastery <= 0.33:
            level = "low"
        elif mastery <= 0.66:
            level = "medium"
        else:
            level = "high"

        action_name = self.action_semantics[action]

        # ã€ä¿®å¤6ã€‘å¢åŠ åŠ¨ä½œæƒ©ç½šï¼ˆé˜²æ­¢åç¼©ï¼‰
        action_penalty = 0.0
        if self.action_counter[action] > 3.0:  # è¿ç»­ä½¿ç”¨åŒä¸€åŠ¨ä½œ3æ¬¡ä»¥ä¸Š
            action_penalty = -0.5 * (self.action_counter[action] - 3.0)

        # ä¿®å¤A: è¿ç»­èµ„æºè¡°å‡æ›¿ä»£ç¡¬è·³å˜
        if action in [0, 1, 2]:  # èµ„æºåŠ¨ä½œ
            res_id = action
            # æ›´æ–°ä½¿ç”¨æ¬¡æ•°
            self.resource_usage_count[res_id] += 1.0
            # è¿ç»­è¡°å‡: reward_scale = exp(-alpha * usage_count)
            usage_count = self.resource_usage_count[res_id]
            resource_decay = np.exp(-self.resource_fatigue_alpha * usage_count)
        else:
            resource_decay = 1.0  # éèµ„æºåŠ¨ä½œä¸è¡°å‡

        # è®¡ç®—å¢ç›Šï¼ˆç¯å¢ƒåŠ¨åŠ›å­¦ï¼‰- åº”ç”¨è¿ç»­è¡°å‡
        base_gain = self.GAIN_MATRIX[level][action_name]
        gain = base_gain * resource_decay

        # è½»å¾®å™ªå£°ï¼Œé¿å…ç­–ç•¥å®Œå…¨ç¡®å®šæ€§ï¼ˆæé«˜ç†µï¼‰
        noise = float(self.np_random.normal(0.0, 0.001))
        new_gap = clamp(prev_gap - gain + noise, 0.0, 1.0)
        self.gap = new_gap

        # å¥–åŠ±ï¼šæœ¬åœ° dense rewardï¼ˆç§»é™¤ç›´æ¥çš„é¦–æ¬¡èµ„æºä½¿ç”¨å¥–åŠ±ï¼‰
        reward = 0.0
        # æŒ‰ gap å‡å°‘é‡ç»™äºˆ dense å¥–åŠ±ï¼Œé¼“åŠ±é€‰æ‹©æ›´å¤§å¹…åº¦åŠ¨ä½œ
        reward += 5.0 * max(0.0, prev_gap - new_gap)
        reward += 5.0 if new_gap < self.tolerance else 0.0
        # ã€ä¿®å¤7ã€‘åœ¨åŸæœ‰rewardåŸºç¡€ä¸ŠåŠ ä¸ŠåŠ¨ä½œæƒ©ç½š
        reward += action_penalty
        reward += -0.01  # æ­¥æƒ©ç½š

        terminated = bool(new_gap < self.tolerance)
        truncated = bool(self.step_count >= self.max_steps)

        # ã€ä¿®å¤8ã€‘è®¡ç®—è§‚å¯Ÿæ—¶åŒ…å«åŠ¨ä½œè®¡æ•°å™¨
        action_counter_norm = np.clip(self.action_counter[action] / 10.0, 0.0, 1.0)
        resource_fatigue = self._compute_resource_fatigue()
        mastery = 1.0 - self.gap
        mastery_category = 0.0 if mastery <= 0.33 else (0.5 if mastery <= 0.66 else 1.0)
        
        obs = np.array([
            self.gap,
            resource_fatigue,
            mastery_category,
            action_counter_norm
        ], dtype=np.float32)
        
        info = {
            "gap": self.gap, 
            "gain": gain, 
            "base_gain": base_gain,
            "resource_decay": resource_decay if action in [0, 1, 2] else 1.0,
            "noise": noise, 
            "mastery": mastery, 
            "level": level, 
            "action_name": action_name,
            "resource_fatigue": resource_fatigue,
            "resource_usage_count": self.resource_usage_count.copy(),
            "action_counter": self.action_counter.copy(),
            "action_penalty": action_penalty
        }
        return obs, reward, terminated, truncated, info


class MiniManagerEnv12(gym.Env):
    """
    12ç»´ç®¡ç†è€…ç¯å¢ƒï¼ˆä¿®å¤ç­–ç•¥åç¼©é—®é¢˜ï¼‰
    
    ä¿®å¤è¯´æ˜ï¼š
    - æ‰©å±•è§‚å¯Ÿç©ºé—´ï¼š[gap_0, ..., gap_11, timer_cat0, timer_cat1, timer_cat2] è§£å†³POMDPé—®é¢˜
    - è¿ç»­é—å¿˜è¡°å‡ï¼šé¿å…ç¡¬è·³å˜å¯¼è‡´çš„ç­–ç•¥åç¼©
    - ä¿ç•™æ‰€æœ‰åŸæœ‰æœºåˆ¶ï¼šé—å¿˜ã€èµ„æºçæƒœã€éš¾åº¦åŒ¹é…ç­‰
    """
    def __init__(self, low_policies=None, tolerance: float = 0.05, max_steps: int = 200, seed: int = 0, 
                 forgetting_mode: str = 'no_forgetting', forgetting_params: dict | None = None, 
                 resource_enabled: bool = False, resource_decay_range: tuple | list = (0.2, 0.4), 
                 match_bonus: float = 0.2, mismatch_penalty: float = 0.1, 
                 timing_bonus: float = 0.03, timing_penalty: float = 0.03, 
                 difficulty_bins: tuple | list = (0.33, 0.66)):
        super().__init__()
        self.low_policies = low_policies or []
        self.tolerance = float(tolerance)
        self.max_steps = int(max_steps)
        self.np_random, _ = gym.utils.seeding.np_random(seed)
        
        # ä¿®å¤B: æ‰©å±•è§‚å¯Ÿç©ºé—´ [gap_0, ..., gap_11, timer_cat0, timer_cat1, timer_cat2]
        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(15,), dtype=np.float32)
        self.action_space = spaces.Discrete(12)
        
        self.delta_map = np.array([0.05, 0.10, 0.20, 0.03, 0.06, 0.02], dtype=np.float32)
        self.gaps = None
        self.step_count = 0
        self._last_action = None
        self.forgetting_mode = str(forgetting_mode)
        
        # ã€ä¿®å¤13ã€‘å¢åŠ é˜²åç¼©æœºåˆ¶
        self.action_penalty_coef = 0.02  # åŠ¨ä½œé‡å¤æƒ©ç½šç³»æ•°
        self.last_actions = []  # è®°å½•æœ€è¿‘åŠ¨ä½œ
        self.action_memory_size = 10  # è®°å¿†æœ€è¿‘10ä¸ªåŠ¨ä½œ
        
        # ã€ä¿®å¤14ã€‘æ ¹æ®ä¸åŒé—å¿˜æ¨¡å¼è°ƒæ•´Timeræ•æ„Ÿæ€§
        # ä½†ä¿æŒBCæ”¶é›†ç­–ç•¥ä¸€è‡´ï¼ˆåªé€šè¿‡ç¯å¢ƒåŠ¨åŠ›å­¦äº§ç”Ÿå·®å¼‚ï¼‰
        self.timer_sensitivity = 1.0  # åŸºç¡€æ•æ„Ÿæ€§
        
        # æ”¹è¿›1: å¼±action-entropy shaping - ç»´æŠ¤æ»‘åŠ¨çª—å£
        self.action_history = []
        self.action_history_window = 50
        self.entropy_coef = 0.01
        
        # æ”¹è¿›2: timer-aware shaping - ç³»æ•°
        self.timer_coef = 0.05
        
        # Categoriesæ˜¯latent environment factorï¼Œagentä¸å¯è§‚æµ‹
        self.categories = {
            'algebra': [0, 1, 2, 3],
            'geometry': [4, 5, 6, 7],
            'statistics': [8, 9, 10, 11],
        }
        self.steps_since_review_cat = {'algebra': 0, 'geometry': 0, 'statistics': 0}
        self.history_buffer_cat = {'algebra': [], 'geometry': [], 'statistics': []}
        self._fp = forgetting_params or {}
        
        # è§¦å‘é¢‘ç‡ç»Ÿè®¡
        self.forgetting_trigger_stats = {'algebra': 0, 'geometry': 0, 'statistics': 0}
        
        # ä¿®å¤B: é—å¿˜é˜ˆå€¼ (ç”¨äºè®¡ç®—å½’ä¸€åŒ–timer)
        self.forgetting_thresholds = {
            'algebra': 4,    # å¢å¼ºFFå‚æ•°
            'geometry': 3,   
            'statistics': 5
        }
        
        self.resource_enabled = bool(resource_enabled)
        self.resource_decay_range = tuple(resource_decay_range)
        self.match_bonus = float(match_bonus)
        self.mismatch_penalty = float(mismatch_penalty)
        self.timing_bonus = float(timing_bonus)
        self.timing_penalty = float(timing_penalty)
        self.difficulty_bins = tuple(difficulty_bins)
        self.resource_used = None

    def reset(self, *, seed: int | None = None, options: dict | None = None):
        if seed is not None:
            self.np_random, _ = gym.utils.seeding.np_random(seed)
        if options and isinstance(options, dict) and 'init_gaps' in options:
            init = np.array(options['init_gaps'], dtype=np.float32)
            if init.shape != (12,):
                init = init.reshape(12,)
            self.gaps = np.clip(init, 0.0, 1.0)
        else:
            self.gaps = self.np_random.uniform(0.2, 0.8, size=(12,)).astype(np.float32)
        self.step_count = 0
        self._last_action = None
        for c in self.steps_since_review_cat:
            self.steps_since_review_cat[c] = 0
        for c in self.history_buffer_cat:
            self.history_buffer_cat[c] = []
        
        # æ”¹è¿›1: é‡ç½®action history
        self.action_history = []
        
        # é‡ç½®é˜²åç¼©æœºåˆ¶
        self.last_actions = []
        
        # ä¿®å¤B: æ‰©å±•è§‚å¯Ÿç©ºé—´åŒ…å«å½’ä¸€åŒ–timer
        timer_obs = self._compute_timer_observation()
        obs = np.concatenate([self.gaps, timer_obs])
        return obs, {}

    def _compute_timer_observation(self):
        """
        ä¿®å¤B: è®¡ç®—å½’ä¸€åŒ–timerè§‚å¯Ÿ (è§£å†³POMDPé—®é¢˜)
        timer_cat = steps_since_review_cat / forgetting_threshold
        ä¸å‘Šè¯‰agentç±»åˆ«å½’å±ï¼Œåªæä¾›timerçŠ¶æ€
        """
        timers = []
        for cat in ['algebra', 'geometry', 'statistics']:
            steps = self.steps_since_review_cat[cat]
            threshold = self.forgetting_thresholds[cat]
            normalized_timer = min(1.0, float(steps) / float(threshold))
            timers.append(normalized_timer)
        return np.array(timers, dtype=np.float32)
    
    def _compute_action_entropy(self):
        """
        æ”¹è¿›1: è®¡ç®—action entropyç”¨äºshaping
        åŸºäºæœ€è¿‘50æ­¥çš„actionåˆ†å¸ƒ
        """
        if len(self.action_history) == 0:
            return 0.0
        
        action_hist = np.zeros(12, dtype=np.float32)
        for act in self.action_history:
            action_hist[int(act)] += 1.0
        
        freq = action_hist / (action_hist.sum() + 1e-8)
        entropy = -np.sum(freq * np.log(freq + 1e-8))
        return float(entropy)
    
    def _compute_timer_sum(self):
        """
        æ”¹è¿›2: è®¡ç®—timeræ€»å’Œç”¨äºtimer-aware shaping
        """
        timer_obs = self._compute_timer_observation()
        return float(np.sum(timer_obs))

    def _global_success(self) -> bool:
        return bool(np.all(self.gaps < self.tolerance))

    def _apply_fixed_forgetting(self, mastery: np.ndarray) -> tuple[np.ndarray, dict]:
        """ä¿®å¤C: è¿ç»­é—å¿˜è¡°å‡çš„å›ºå®šé—å¿˜æœºåˆ¶"""
        # å¢å¼ºå‚æ•°ï¼ˆç»Ÿä¸€å¢å¼ºï¼‰
        enhanced_params = {
            'thresholds': {
                'algebra': 4,    # ä»6å‡å°‘åˆ°4ï¼ˆæ›´æ—©è§¦å‘é—å¿˜ï¼‰
                'geometry': 3,   # ä»4å‡å°‘åˆ°3ï¼ˆæ›´æ—©è§¦å‘é—å¿˜ï¼‰  
                'statistics': 5  # ä»7å‡å°‘åˆ°5ï¼ˆæ›´æ—©è§¦å‘é—å¿˜ï¼‰
            },
            'strengths': {
                'algebra': 0.08,     # ä»0.036å¢åŠ åˆ°0.08ï¼ˆ2.2å€å¼ºåº¦ï¼‰
                'geometry': 0.08,    # ä»0.036å¢åŠ åˆ°0.08ï¼ˆ2.2å€å¼ºåº¦ï¼‰
                'statistics': 0.08   # ä»0.036å¢åŠ åˆ°0.08ï¼ˆ2.2å€å¼ºåº¦ï¼‰
            },
            'adjust_factor': 0.7,    # ä»0.5å¢åŠ åˆ°0.7ï¼ˆå¢å¼ºå½±å“ï¼‰
        }
        
        enhanced_params.update(self._fp.get('fixed', {}))
        events = {k: False for k in self.categories}
        
        for cat, idxs in self.categories.items():
            t = int(self.steps_since_review_cat[cat])
            th = int(enhanced_params['thresholds'][cat])
            
            # ä¿®å¤C: è¿ç»­é—å¿˜è¡°å‡ (é¿å…ç¡¬è·³å˜)
            if t >= th:
                s = float(enhanced_params['strengths'][cat])
                adj = float(enhanced_params.get('adjust_factor', 0.5))
                
                # è¿ç»­è¡°å‡: forgetting_strength = min(1.0, (timer - threshold) / scale)
                excess_steps = max(0, t - th)
                scale = 5.0  # æ§åˆ¶è¡°å‡å¹³æ»‘åº¦
                forgetting_strength = min(1.0, float(excess_steps) / scale)
                
                # åº”ç”¨è¿ç»­è¡°å‡
                effective_decay = 1.0 - (s * adj * forgetting_strength)
                
                for i in idxs:
                    mastery[i] = float(max(0.0, mastery[i] * effective_decay))
                
                events[cat] = True
                self.forgetting_trigger_stats[cat] += 1
                
        return mastery, events

    def _apply_improved_forgetting(self, mastery: np.ndarray) -> tuple[np.ndarray, dict]:
        """ä¿®å¤C: è¿ç»­é—å¿˜è¡°å‡çš„æ”¹è¿›é—å¿˜æœºåˆ¶"""
        # ä½¿ç”¨ä¸å¢å¼ºFFç›¸åŒçš„è§¦å‘é˜ˆå€¼ï¼ˆåŒæ­¥å¢å¼ºï¼Œä¿æŒä¸FFä¸€è‡´ï¼‰
        thresholds = {
            'algebra': 4,    # ä¸FFä¿æŒä¸€è‡´
            'geometry': 3,   # ä¸FFä¿æŒä¸€è‡´
            'statistics': 5  # ä¸FFä¿æŒä¸€è‡´
        }
        
        # IFç‰¹æœ‰å‚æ•° - Stability-Aware Threshold Forgetting
        if_params = {
            'base_decay': 0.08,        # ä¸FFåŸºç¡€å¼ºåº¦ä¸€è‡´
            'stability_k': 8.0,         # ç¨³å®šæ€§è®¡ç®—å‚æ•°ï¼ˆä¿æŒä¸å˜ï¼‰
            'stability_lambda': 0.7,    # ç¨³å®šæ€§å½±å“å› å­ï¼ˆä¿æŒä¸å˜ï¼‰
        }
        if_params.update(self._fp.get('improved', {}))
        
        ed = {k: 0.0 for k in self.categories}  # è®°å½•å®é™…è¡°å‡å¼ºåº¦
        
        for cat, idxs in self.categories.items():
            t_c = int(self.steps_since_review_cat[cat])
            th = int(thresholds[cat])
            
            # é˜ˆå€¼è§¦å‘ (ä¸FFå®Œå…¨ä¸€è‡´çš„è§¦å‘æ¡ä»¶)
            if t_c >= th:
                # è®¡ç®—å†å²ç¨³å®šæ€§
                hist = list(self.history_buffer_cat.get(cat, []))
                if len(hist) >= 2:
                    vol_c = float(np.std(hist))
                    stability_c = float(np.exp(-if_params['stability_k'] * vol_c))
                else:
                    stability_c = 0.5  # é»˜è®¤ä¸­ç­‰ç¨³å®šæ€§
                
                # Stability-Aware Threshold Forgetting æ ¸å¿ƒåˆ›æ–°
                base_decay = if_params['base_decay']
                lambda_factor = if_params['stability_lambda']
                
                # ä¿®å¤C: è¿ç»­è¡°å‡ + ç¨³å®šæ€§æ„ŸçŸ¥
                excess_steps = max(0, t_c - th)
                scale = 5.0  # æ§åˆ¶è¡°å‡å¹³æ»‘åº¦
                forgetting_strength = min(1.0, float(excess_steps) / scale)
                
                # IFçš„å…³é”®ä¸€åˆ€: effective_decay = base_decay * (1 - Î» * stability_c) * forgetting_strength
                effective_decay_base = base_decay * (1.0 - lambda_factor * stability_c)
                effective_decay = effective_decay_base * forgetting_strength
                
                # åº”ç”¨é—å¿˜ (ä½¿ç”¨adjust_factorä¿æŒä¸FFä¸€è‡´çš„åº”ç”¨æ–¹å¼)
                decay_multiplier = 1.0 - (effective_decay * 0.7)  # 0.7æ˜¯adjust_factor
                for i in idxs:
                    mastery[i] = float(max(0.0, mastery[i] * decay_multiplier))
                
                ed[cat] = effective_decay
                self.forgetting_trigger_stats[cat] += 1
                
        return mastery, ed

    def step(self, action: int):
        self.step_count += 1
        prev_sum = float(np.sum(self.gaps))
        prev_completed = int(np.sum(self.gaps < self.tolerance))
        
        skill_id = int(action)
        skill_gap = float(self.gaps[skill_id])
        
        # ç¡®ä¿å¿…é¡»æœ‰12ä¸ªä½å±‚ç­–ç•¥
        assert len(self.low_policies) == 12, "å¿…é¡»æä¾›12ä¸ªä½å±‚ç­–ç•¥"
        # ä¿®å¤A: æ‰©å±•ä½å±‚è§‚å¯Ÿç©ºé—´ [gap, resource_fatigue, mastery_category, action_counter]
        resource_fatigue = 0.0  # é«˜å±‚ä¸ç›´æ¥ç®¡ç†èµ„æºç–²åŠ³ï¼Œè®¾ä¸º0
        mastery = 1.0 - skill_gap
        mastery_category = 0.0 if mastery <= 0.33 else (0.5 if mastery <= 0.66 else 1.0)
        obs_skill = np.array([skill_gap, resource_fatigue, mastery_category, 0.0], dtype=np.float32)
        low_policy = self.low_policies[skill_id]
        act, _ = low_policy.predict(obs_skill, deterministic=True)
        type_id = int(act)
        
        # è®¡ç®—æŒæ¡åº¦å’Œéš¾åº¦çº§åˆ« (ä¸MiniSkillEnvä¸€è‡´)
        mastery = 1.0 - skill_gap
        if mastery <= 0.33:
            level = "low"
        elif mastery <= 0.66:
            level = "medium"
        else:
            level = "high"
        
        # åŠ¨ä½œè¯­ä¹‰å®šä¹‰ (ä¸MiniSkillEnvä¸€è‡´)
        action_semantics = [
            "easy_res", "medium_res", "hard_res",
            "seek_teacher", "peer_discussion", "self_learn"
        ]
        
        # å¢ç›ŠçŸ©é˜µ (ä¸MiniSkillEnvå®Œå…¨ä¸€è‡´)
        GAIN_MATRIX = {
            "low": {
                "easy_res": 0.1, "medium_res": 0.05, "hard_res": 0.03,
                "seek_teacher": 0.05, "peer_discussion": 0.025, "self_learn": 0.015
            },
            "medium": {
                "easy_res": 0.05, "medium_res": 0.1, "hard_res": 0.05,
                "seek_teacher": 0.025, "peer_discussion": 0.05, "self_learn": 0.025
            },
            "high": {
                "easy_res": 0.03, "medium_res": 0.05, "hard_res": 0.1,
                "seek_teacher": 0.015, "peer_discussion": 0.025, "self_learn": 0.05
            }
        }
        
        # ä½¿ç”¨GAIN_MATRIXè®¡ç®—å¢ç›Š (ä¸MiniSkillEnvä¸€è‡´)
        action_name = action_semantics[type_id]
        gain = GAIN_MATRIX[level][action_name]
        
        # å™ªå£° (ä¸MiniSkillEnvä¸€è‡´)
        noise = float(self.np_random.normal(0.0, 0.001))
        
        # è®¡ç®—æ–°çš„gap (ä¸MiniSkillEnvä¸€è‡´)
        delta = 0.0 if (skill_gap < self.tolerance) else gain
        new_gap = clamp(skill_gap - delta + noise, 0.0, 1.0)
        self.gaps[skill_id] = new_gap
        
        # æ›´æ–°é—å¿˜æœºåˆ¶çŠ¶æ€
        for cat, idxs in self.categories.items():
            if skill_id in idxs:
                self.steps_since_review_cat[cat] = 0
            else:
                self.steps_since_review_cat[cat] += 1
                
        # åº”ç”¨é—å¿˜æœºåˆ¶
        mastery = 1.0 - self.gaps.copy()
        mastery_before = mastery.copy()
        forgetting_event = {'algebra': False, 'geometry': False, 'statistics': False}
        effective_decay_cat = {'algebra': 0.0, 'geometry': 0.0, 'statistics': 0.0}
        
        if self.forgetting_mode == 'fixed_forgetting':
            mastery, fe = self._apply_fixed_forgetting(mastery)
            forgetting_event.update(fe)
        elif self.forgetting_mode == 'improved_forgetting':
            mastery, ed = self._apply_improved_forgetting(mastery)
            effective_decay_cat.update(ed)
        
        # æ›´æ–°gaps
        self.gaps = np.clip(1.0 - mastery, 0.0, 1.0)
        
        # æ”¹è¿›1: æ›´æ–°action historyç”¨äºentropyè®¡ç®—
        self.action_history.append(skill_id)
        if len(self.action_history) > self.action_history_window:
            self.action_history = self.action_history[-self.action_history_window:]
        
        # ã€ä¿®å¤15ã€‘åŠ¨ä½œé‡å¤æƒ©ç½š
        repetition_penalty = 0.0
        if len(self.last_actions) >= 2:
            # å¦‚æœæœ€è¿‘é¢‘ç¹é€‰æ‹©åŒä¸€åŠ¨ä½œï¼Œç»™äºˆæƒ©ç½š
            recent_actions = self.last_actions[-5:] if len(self.last_actions) >= 5 else self.last_actions
            same_action_count = sum(1 for a in recent_actions if a == skill_id)
            if same_action_count >= 3:  # æœ€è¿‘5æ­¥å†…é€‰æ‹©åŒä¸€åŠ¨ä½œ3æ¬¡ä»¥ä¸Š
                repetition_penalty = -self.action_penalty_coef * same_action_count
        
        # æ›´æ–°åŠ¨ä½œè®°å¿†
        self.last_actions.append(skill_id)
        if len(self.last_actions) > self.action_memory_size:
            self.last_actions.pop(0)
        
        # è®¡ç®—å¥–åŠ±
        curr_sum = float(np.sum(self.gaps))
        curr_completed = int(np.sum(self.gaps < self.tolerance))
        delta_sum = prev_sum - curr_sum
        
        # åŸºç¡€å¥–åŠ±ç»“æ„
        reward = 0.0
        reward += 5.0 * delta_sum  # gapå‡å°‘å¥–åŠ±
        reward += 5.0 * max(0, curr_completed - prev_completed)
        reward += 20.0 if self._global_success() else 0.0  # å…¨å±€æˆåŠŸå¥–åŠ±
        
        # ã€ä¿®å¤15ã€‘æ·»åŠ åŠ¨ä½œé‡å¤æƒ©ç½š
        reward += repetition_penalty
        
        # ã€ä¿®å¤16ã€‘å‡è¡¡æ€§å¥–åŠ±ï¼ˆé¼“åŠ±è¦†ç›–æ‰€æœ‰æŠ€èƒ½ï¼‰
        completed_skills = np.sum(self.gaps < self.tolerance)
        balance_reward = 0.1 * (completed_skills / 12.0)  # å®Œæˆè¶Šå¤šå¥–åŠ±è¶Šé«˜
        reward += balance_reward
        
        # ã€ä¿®å¤18ã€‘å¢åŠ ç†µå¥–åŠ±ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰
        current_entropy = self._compute_action_entropy()
        entropy_bonus = 0.03 * current_entropy  # å°å¹…ç†µå¥–åŠ±
        reward += entropy_bonus
        
        reward += -0.02  # æ­¥æƒ©ç½š
        
        # ã€EXP15ä¿®å¤ã€‘å¼ºåˆ¶ç­–ç•¥å…³æ³¨æ‰€æœ‰ç»´åº¦ - ç»´åº¦å‡è¡¡æ€§æƒ©ç½š
        gap_variance = np.var(self.gaps)
        if gap_variance > 0.05:  # é˜ˆå€¼å¯è°ƒï¼Œæ–¹å·®å¤§åˆ™æƒ©ç½šé‡
            reward -= 5.0 * gap_variance  # æƒ©ç½šä¸å‡è¡¡çš„å­¦ä¹ 
        else:
            reward += 0.5  # å¥–åŠ±å‡è¡¡çš„å­¦ä¹ 
        
        # ã€EXP15ä¿®å¤ã€‘Timeræƒ©ç½š - æé«˜æƒ©ç½šç³»æ•°ï¼Œè¿«ä½¿ç­–ç•¥å¿…é¡»è½®æ¢
        timer_penalty_coef = 0.0  # é»˜è®¤æ— æƒ©ç½š
        if self.forgetting_mode in ['fixed_forgetting', 'improved_forgetting']:
            # åªæœ‰åœ¨æœ‰é—å¿˜æœºåˆ¶æ—¶ï¼ŒTimeré«˜äº†æ‰æ‰£åˆ†
            # è¿™ä¼šé€¼è¿«Agentå»å‹ä½Timerï¼ˆå³è½®æ¢å¤ä¹ ï¼‰
            timer_penalty_coef = 0.5  # ã€EXP15ä¿®å¤ã€‘æé«˜åˆ°0.5ï¼Œè¿«ä½¿ç­–ç•¥å¿…é¡»è½®æ¢
        
        timer_sum = self._compute_timer_sum()
        reward -= timer_penalty_coef * timer_sum  # æ–°å¢Timeræƒ©ç½š
        
        # æ”¹è¿›1: å¼±action-entropy shaping (æ‰€æœ‰Managerè®­ç»ƒ)
        entropy = self._compute_action_entropy()
        reward += self.entropy_coef * entropy
        
        terminated = self._global_success()
        truncated = bool(self.step_count >= self.max_steps)
        self._last_action = skill_id
        
        # åŠ¨ä½œæ©ç ï¼š12ä¸ªåŠ¨ä½œ
        mask = (self.gaps >= self.tolerance).astype(np.int32)
        
        # ä¿®å¤B: æ‰©å±•è§‚å¯Ÿç©ºé—´åŒ…å«å½’ä¸€åŒ–timer
        timer_obs = self._compute_timer_observation()
        obs = np.concatenate([self.gaps, timer_obs])
        
        # æ”¹è¿›6: è¯Šæ–­æ—¥å¿—
        gap_variance = float(np.var(self.gaps))
        action_freq = np.zeros(12, dtype=np.float32)
        for act in self.action_history:
            action_freq[int(act)] += 1.0
        action_freq = action_freq / (action_freq.sum() + 1e-8)
        
        info = {
            # åŸºç¡€æŒ‡æ ‡
            'sum_prev': prev_sum,
            'sum_curr': curr_sum,
            'skill_id': skill_id,
            'low_action': int(type_id),
            'delta': delta,
            'noise': noise,
            'action_mask': mask.tolist(),
            'forgetting_mode': self.forgetting_mode,
            
            # å…³é”®è¯Šæ–­æŒ‡æ ‡
            'gap_variance': gap_variance,  # å­¦ä¹ å‡è¡¡æ€§
            'action_entropy': entropy,  # ç­–ç•¥æ¢ç´¢æ°´å¹³
            'timer_mean': float(np.mean(timer_obs)),  # timerçŠ¶æ€
            'timer_std': float(np.std(timer_obs)),  # timeræ³¢åŠ¨æ€§
            
            # é—å¿˜ç›¸å…³
            'forgetting_triggers': dict(self.forgetting_trigger_stats),  # å„ç±»åˆ«è§¦å‘æ¬¡æ•°
            'effective_decay_cat': effective_decay_cat,  # å®é™…è¡°å‡å¼ºåº¦
            'steps_since_review_cat': dict(self.steps_since_review_cat),  # è·ç¦»ä¸Šæ¬¡å¤ä¹ 
            'forgetting_event': forgetting_event,  # æœ¬æ¬¡æ˜¯å¦è§¦å‘
            
            # ç­–ç•¥åˆ†æ
            'action_freq': action_freq.tolist(),  # åŠ¨ä½œä½¿ç”¨é¢‘ç‡
            'gap_mean': float(np.mean(self.gaps)),  # å¹³å‡gap
            
            # ä¿ç•™åŸæœ‰å­—æ®µï¼ˆå‘åå…¼å®¹ï¼‰
            'timer_obs': timer_obs.tolist(),
            'mastery_before': mastery_before.tolist(),
            'mastery_after': mastery.tolist(),
            'timer_sum': timer_sum,
        }

        # ã€EXP15ä¿®å¤ã€‘å¼ºåˆ¶ç­–ç•¥å…³æ³¨æ‰€æœ‰ç»´åº¦ - é˜²æ­¢åç¼©
        gap_variance = float(np.var(self.gaps))
        if gap_variance > 0.05:
            reward -= 5.0 * gap_variance  # æƒ©ç½šä¸å‡è¡¡çš„å­¦ä¹ 
        else:
            reward += 0.5  # å¥–åŠ±å‡è¡¡çš„å­¦ä¹ 
        
        # æ›´æ–°å†å²ç¼“å†²åŒº
        for cat, idxs in self.categories.items():
            M_c = float(np.mean(mastery[idxs]))
            hb = self.history_buffer_cat.get(cat, [])
            hb = hb + [M_c]
            if len(hb) > 50:
                hb = hb[-50:]
            self.history_buffer_cat[cat] = hb
            
        return obs, reward, terminated, truncated, info


class FlatMiniEnv12(gym.Env):
    """
    12ç»´ Flat PPO ç¯å¢ƒï¼ˆä¿®å¤ç­–ç•¥åç¼©é—®é¢˜ï¼‰
    
    ä¿®å¤è¯´æ˜ï¼š
    - æ‰©å±•è§‚å¯Ÿç©ºé—´ï¼š[gap_0, ..., gap_11, timer_cat0, timer_cat1, timer_cat2] è§£å†³POMDPé—®é¢˜
    - è¿ç»­é—å¿˜è¡°å‡ï¼šé¿å…ç¡¬è·³å˜å¯¼è‡´çš„ç­–ç•¥åç¼©
    - ä¿ç•™æ‰€æœ‰åŸæœ‰æœºåˆ¶ï¼šé—å¿˜ã€èµ„æºçæƒœã€éš¾åº¦åŒ¹é…ç­‰
    """

    def __init__(self, tolerance: float = 0.05, max_steps: int = 200, seed: int = 0, 
                 forgetting_mode: str = 'no_forgetting', forgetting_params: dict | None = None,
                 resource_enabled: bool = False, resource_decay_range: tuple | list = (0.2, 0.4), 
                 match_bonus: float = 0.2, mismatch_penalty: float = 0.1, 
                 timing_bonus: float = 0.03, timing_penalty: float = 0.03, 
                 difficulty_bins: tuple | list = (0.33, 0.66)):
        super().__init__()
        self.tolerance = float(tolerance)
        self.max_steps = int(max_steps)
        self.np_random, _ = gym.utils.seeding.np_random(seed)
        
        # ä¿®å¤B: æ‰©å±•è§‚å¯Ÿç©ºé—´ [gap_0, ..., gap_11, timer_cat0, timer_cat1, timer_cat2]
        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(15,), dtype=np.float32)
        self.action_space = spaces.Discrete(72)  # 12 skills Ã— 6 actions
        
        # ğŸ”§ ä¿®å¤ä¸å…¬å¹³ç‚¹1: ä½¿ç”¨ä¸åˆ†å±‚ç¯å¢ƒç›¸åŒçš„GAIN_MATRIX
        # åŠ¨ä½œè¯­ä¹‰å®šä¹‰ (ä¸MiniSkillEnvä¿æŒä¸€è‡´)
        self.action_semantics = [
            "easy_res", "medium_res", "hard_res",
            "seek_teacher", "peer_discussion", "self_learn"
        ]
        
        # å¢ç›ŠçŸ©é˜µï¼ˆä¸MiniSkillEnvå®Œå…¨ä¸€è‡´çš„ç¯å¢ƒåŠ¨åŠ›å­¦ï¼‰
        self.GAIN_MATRIX = {
            "low": {        # æŒæ¡åº¦ 0.00-0.33
                "easy_res": 0.1, "medium_res": 0.05, "hard_res": 0.03,
                "seek_teacher": 0.05, "peer_discussion": 0.025, "self_learn": 0.015
            },
            "medium": {     # æŒæ¡åº¦ 0.34-0.66
                "easy_res": 0.05, "medium_res": 0.1, "hard_res": 0.05,
                "seek_teacher": 0.025, "peer_discussion": 0.05, "self_learn": 0.025
            },
            "high": {       # æŒæ¡åº¦ 0.67-1.00
                "easy_res": 0.03, "medium_res": 0.05, "hard_res": 0.1,
                "seek_teacher": 0.015, "peer_discussion": 0.025, "self_learn": 0.05
            }
        }
        
        # ä¿ç•™åŸæœ‰delta_mapä½œä¸ºå¤‡ç”¨ï¼ˆå‘åå…¼å®¹ï¼‰
        self.delta_map = np.array([0.05, 0.10, 0.20, 0.03, 0.06, 0.02], dtype=np.float32)
        
        # ğŸ”§ ä¿®å¤ä¸å…¬å¹³ç‚¹1: æ·»åŠ èµ„æºç–²åŠ³æœºåˆ¶ (ä¸MiniSkillEnvä¿æŒä¸€è‡´)
        self.resource_usage_count = np.zeros((12, 3), dtype=np.float32)  # 12ä¸ªæŠ€èƒ½ Ã— 3ä¸ªèµ„æºåŠ¨ä½œ
        self.resource_fatigue_alpha = 0.5  # ä¸MiniSkillEnvä¸€è‡´
        
        self.gaps = None
        self.step_count = 0
        self._last_action = None
        
        # é—å¿˜æœºåˆ¶æ”¯æŒ
        self.forgetting_mode = str(forgetting_mode)
        self.categories = {
            'algebra': [0, 1, 2, 3],
            'geometry': [4, 5, 6, 7],
            'statistics': [8, 9, 10, 11],
        }
        self.steps_since_review_cat = {'algebra': 0, 'geometry': 0, 'statistics': 0}
        self.history_buffer_cat = {'algebra': [], 'geometry': [], 'statistics': []}
        self._fp = forgetting_params or {}
        
        # è§¦å‘é¢‘ç‡ç»Ÿè®¡
        self.forgetting_trigger_stats = {'algebra': 0, 'geometry': 0, 'statistics': 0}
        
        # ä¿®å¤B: é—å¿˜é˜ˆå€¼ (ç”¨äºè®¡ç®—å½’ä¸€åŒ–timer)
        self.forgetting_thresholds = {
            'algebra': 4,    # å¢å¼ºFFå‚æ•°
            'geometry': 3,   
            'statistics': 5
        }
        
        # èµ„æºæœºåˆ¶æ”¯æŒ
        self.resource_enabled = bool(resource_enabled)
        self.resource_decay_range = tuple(resource_decay_range)
        self.match_bonus = float(match_bonus)
        self.mismatch_penalty = float(mismatch_penalty)
        self.timing_bonus = float(timing_bonus)
        self.timing_penalty = float(timing_penalty)
        self.difficulty_bins = tuple(difficulty_bins)
        self.resource_used = None
        
        # æ”¹è¿›1: å¼±action-entropy shaping - ç»´æŠ¤æ»‘åŠ¨çª—å£ (Flatä¹Ÿä½¿ç”¨)
        self.action_history = []
        self.action_history_window = 50
        # ğŸ”§ ä¿®å¤ä¸å…¬å¹³ç‚¹3: è°ƒæ•´entropyç³»æ•°ä»¥åŒ¹é…åŠ¨ä½œç©ºé—´ç»´åº¦
        # åˆ†å±‚æ˜¯12ç»´ï¼Œæ‰å¹³æ˜¯72ç»´ï¼Œæ‰€ä»¥æ‰å¹³çš„ç³»æ•°åº”è¯¥æ˜¯ 0.01 * 72/12 = 0.06
        self.entropy_coef = 0.06  # è°ƒæ•´ä¸º6å€ï¼Œè¡¥å¿72ç»´åŠ¨ä½œç©ºé—´çš„ç¨€é‡Šæ•ˆåº”

    def reset(self, *, seed: int | None = None, options: dict | None = None):
        if seed is not None:
            self.np_random, _ = gym.utils.seeding.np_random(seed)
        
        # åˆå§‹åŒ–12ç»´gaps
        if options and isinstance(options, dict) and 'init_gaps' in options:
            init = np.array(options['init_gaps'], dtype=np.float32)
            if init.shape != (12,):
                init = init.reshape(12,)
            self.gaps = np.clip(init, 0.0, 1.0)
        else:
            self.gaps = self.np_random.uniform(0.2, 0.8, size=(12,)).astype(np.float32)
        
        self.step_count = 0
        self._last_action = None
        
        # é‡ç½®é—å¿˜æœºåˆ¶çŠ¶æ€
        for c in self.steps_since_review_cat:
            self.steps_since_review_cat[c] = 0
        for c in self.forgetting_trigger_stats:
            self.forgetting_trigger_stats[c] = 0
        for c in self.history_buffer_cat:
            self.history_buffer_cat[c] = []
        
        # ğŸ”§ ä¿®å¤ä¸å…¬å¹³ç‚¹1: é‡ç½®èµ„æºç–²åŠ³åº¦
        self.resource_usage_count.fill(0.0)
        
        # æ”¹è¿›1: é‡ç½®action history
        self.action_history = []
        
        # ä¿®å¤B: æ‰©å±•è§‚å¯Ÿç©ºé—´åŒ…å«å½’ä¸€åŒ–timer
        timer_obs = self._compute_timer_observation()
        obs = np.concatenate([self.gaps, timer_obs])
        return obs, {}

    def _compute_timer_observation(self):
        """
        ä¿®å¤B: è®¡ç®—å½’ä¸€åŒ–timerè§‚å¯Ÿ (è§£å†³POMDPé—®é¢˜)
        """
        timers = []
        for cat in ['algebra', 'geometry', 'statistics']:
            steps = self.steps_since_review_cat[cat]
            threshold = self.forgetting_thresholds[cat]
            normalized_timer = min(1.0, float(steps) / float(threshold))
            timers.append(normalized_timer)
        return np.array(timers, dtype=np.float32)
    
    def _compute_action_entropy(self):
        """
        æ”¹è¿›1: è®¡ç®—action entropyç”¨äºshaping
        åŸºäºæœ€è¿‘50æ­¥çš„actionåˆ†å¸ƒ (Flatä½¿ç”¨72ä¸ªåŠ¨ä½œ)
        """
        if len(self.action_history) == 0:
            return 0.0
        
        action_hist = np.zeros(72, dtype=np.float32)
        for act in self.action_history:
            action_hist[int(act)] += 1.0
        
        freq = action_hist / (action_hist.sum() + 1e-8)
        entropy = -np.sum(freq * np.log(freq + 1e-8))
        return float(entropy)

    def _global_success(self) -> bool:
        return bool(np.all(self.gaps < self.tolerance))

    def _apply_fixed_forgetting(self, mastery: np.ndarray) -> tuple[np.ndarray, dict]:
        """ä¿®å¤C: è¿ç»­é—å¿˜è¡°å‡çš„å›ºå®šé—å¿˜æœºåˆ¶"""
        enhanced_params = {
            'thresholds': {
                'algebra': 4, 'geometry': 3, 'statistics': 5  # ä¸åˆ†å±‚ç¯å¢ƒå®Œå…¨ä¸€è‡´
            },
            'strengths': {
                'algebra': 0.08, 'geometry': 0.08, 'statistics': 0.08  # ä¸åˆ†å±‚ç¯å¢ƒå®Œå…¨ä¸€è‡´
            },
            'adjust_factor': 0.7,  # ä¸åˆ†å±‚ç¯å¢ƒå®Œå…¨ä¸€è‡´
        }
        
        enhanced_params.update(self._fp.get('fixed', {}))
        events = {k: False for k in self.categories}
        
        for cat, idxs in self.categories.items():
            t = int(self.steps_since_review_cat[cat])
            th = int(enhanced_params['thresholds'][cat])
            
            if t >= th:
                s = float(enhanced_params['strengths'][cat])
                adj = float(enhanced_params.get('adjust_factor', 0.5))
                
                # ä¿®å¤C: è¿ç»­è¡°å‡
                excess_steps = max(0, t - th)
                scale = 5.0
                forgetting_strength = min(1.0, float(excess_steps) / scale)
                effective_decay = 1.0 - (s * adj * forgetting_strength)
                
                for i in idxs:
                    mastery[i] = float(max(0.0, mastery[i] * effective_decay))
                
                events[cat] = True
                self.forgetting_trigger_stats[cat] += 1
                
        return mastery, events

    def _apply_improved_forgetting(self, mastery: np.ndarray) -> tuple[np.ndarray, dict]:
        """ä¿®å¤C: è¿ç»­é—å¿˜è¡°å‡çš„æ”¹è¿›é—å¿˜æœºåˆ¶"""
        thresholds = {'algebra': 4, 'geometry': 3, 'statistics': 5}  # ä¸åˆ†å±‚ç¯å¢ƒå®Œå…¨ä¸€è‡´
        
        if_params = {
            'base_decay': 0.08,  # ä¸åˆ†å±‚ç¯å¢ƒå®Œå…¨ä¸€è‡´
            'stability_k': 8.0,
            'stability_lambda': 0.7,
        }
        if_params.update(self._fp.get('improved', {}))
        
        ed = {k: 0.0 for k in self.categories}
        
        for cat, idxs in self.categories.items():
            t_c = int(self.steps_since_review_cat[cat])
            th = int(thresholds[cat])
            
            if t_c >= th:
                hist = list(self.history_buffer_cat.get(cat, []))
                if len(hist) >= 2:
                    vol_c = float(np.std(hist))
                    stability_c = float(np.exp(-if_params['stability_k'] * vol_c))
                else:
                    stability_c = 0.5
                
                base_decay = if_params['base_decay']
                lambda_factor = if_params['stability_lambda']
                
                # ä¿®å¤C: è¿ç»­è¡°å‡ + ç¨³å®šæ€§æ„ŸçŸ¥
                excess_steps = max(0, t_c - th)
                scale = 5.0
                forgetting_strength = min(1.0, float(excess_steps) / scale)
                
                effective_decay_base = base_decay * (1.0 - lambda_factor * stability_c)
                effective_decay = effective_decay_base * forgetting_strength
                
                decay_multiplier = 1.0 - (effective_decay * 0.7)  # ä¸åˆ†å±‚ç¯å¢ƒå®Œå…¨ä¸€è‡´
                for i in idxs:
                    mastery[i] = float(max(0.0, mastery[i] * decay_multiplier))
                
                ed[cat] = effective_decay
                self.forgetting_trigger_stats[cat] += 1
                
        return mastery, ed

    def step(self, action: int):
        self.step_count += 1
        prev_sum = float(np.sum(self.gaps))
        prev_completed = int(np.sum(self.gaps < self.tolerance))

        # è§£ç åŠ¨ä½œï¼š72ä¸ªåŠ¨ä½œ = 12ä¸ªæŠ€èƒ½ Ã— 6ä¸ªåŠ¨ä½œç±»å‹
        skill_id = int(action // 6)
        type_id = int(action % 6)
        
        # ğŸ”§ ä¿®å¤ä¸å…¬å¹³ç‚¹1: ä½¿ç”¨ä¸åˆ†å±‚ç¯å¢ƒç›¸åŒçš„difficulty-awareåŠ¨åŠ›å­¦
        skill_gap = float(self.gaps[skill_id])
        mastery = 1.0 - skill_gap
        
        # ç¡®å®šæŒæ¡åº¦åŒºé—´ (ä¸MiniSkillEnvå®Œå…¨ä¸€è‡´)
        if mastery <= 0.33:
            level = "low"
        elif mastery <= 0.66:
            level = "medium"
        else:
            level = "high"
        
        action_name = self.action_semantics[type_id]
        
        # ğŸ”§ ä¿®å¤ä¸å…¬å¹³ç‚¹1: æ·»åŠ èµ„æºè¡°å‡æœºåˆ¶ (ä¸MiniSkillEnvä¿æŒä¸€è‡´)
        resource_decay = 1.0
        if type_id in [0, 1, 2]:  # èµ„æºåŠ¨ä½œ
            res_id = type_id
            # æ›´æ–°ä½¿ç”¨æ¬¡æ•°
            self.resource_usage_count[skill_id, res_id] += 1.0
            # è¿ç»­è¡°å‡: resource_decay = exp(-alpha * usage_count)
            usage_count = self.resource_usage_count[skill_id, res_id]
            resource_decay = np.exp(-self.resource_fatigue_alpha * usage_count)
        
        # ä½¿ç”¨GAIN_MATRIXè®¡ç®—å¢ç›Š (ä¸åˆ†å±‚ç¯å¢ƒå®Œå…¨ä¸€è‡´çš„åŠ¨åŠ›å­¦)
        base_gain = self.GAIN_MATRIX[level][action_name]
        # åº”ç”¨èµ„æºè¡°å‡
        gain = base_gain * resource_decay
        
        # ğŸ”§ ä¿®å¤ä¸å…¬å¹³ç‚¹2: ä½¿ç”¨ä¸åˆ†å±‚ç¯å¢ƒç›¸åŒçš„å™ªå£°å¼ºåº¦
        noise = float(self.np_random.normal(0.0, 0.001))  # ä»0.005æ”¹ä¸º0.001
        
        # è®¡ç®—gapå˜åŒ– (ä½¿ç”¨difficulty-aware gain + èµ„æºè¡°å‡)
        delta = 0.0 if (skill_gap < self.tolerance) else gain
        new_gap = clamp(skill_gap - delta + noise, 0.0, 1.0)
        self.gaps[skill_id] = new_gap
        
        # æ›´æ–°é—å¿˜æœºåˆ¶çŠ¶æ€
        for cat, idxs in self.categories.items():
            if skill_id in idxs:
                self.steps_since_review_cat[cat] = 0
            else:
                self.steps_since_review_cat[cat] += 1

        # åº”ç”¨é—å¿˜æœºåˆ¶
        mastery = 1.0 - self.gaps.copy()
        mastery_before = mastery.copy()
        forgetting_event = {'algebra': False, 'geometry': False, 'statistics': False}
        effective_decay_cat = {'algebra': 0.0, 'geometry': 0.0, 'statistics': 0.0}
        
        if self.forgetting_mode == 'fixed_forgetting':
            mastery, fe = self._apply_fixed_forgetting(mastery)
            forgetting_event.update(fe)
        elif self.forgetting_mode == 'improved_forgetting':
            mastery, ed = self._apply_improved_forgetting(mastery)
            effective_decay_cat.update(ed)
        
        # æ›´æ–°gaps
        self.gaps = np.clip(1.0 - mastery, 0.0, 1.0)
        
        # æ”¹è¿›1: æ›´æ–°action historyç”¨äºentropyè®¡ç®—
        self.action_history.append(action)
        if len(self.action_history) > self.action_history_window:
            self.action_history = self.action_history[-self.action_history_window:]
        
        # è®¡ç®—å¥–åŠ±
        curr_sum = float(np.sum(self.gaps))
        curr_completed = int(np.sum(self.gaps < self.tolerance))
        delta_sum = prev_sum - curr_sum
        
        reward = 0.0
        reward += 5.0 * delta_sum
        reward += 5.0 * max(0, curr_completed - prev_completed)
        reward += 20.0 if self._global_success() else 0.0
        reward += -0.02
        
        # æ”¹è¿›1: å¼±action-entropy shaping (Flatä¹Ÿä½¿ç”¨ï¼Œä½†ä¸ä½¿ç”¨timer-aware shaping)
        entropy = self._compute_action_entropy()
        reward += self.entropy_coef * entropy

        
        # ã€EXP15ä¿®å¤ã€‘Timeræƒ©ç½š - æé«˜æƒ©ç½šç³»æ•°ï¼Œè¿«ä½¿ç­–ç•¥å¿…é¡»è½®æ¢
        timer_penalty_coef = 0.0  # é»˜è®¤æ— æƒ©ç½š
        if self.forgetting_mode in ['fixed_forgetting', 'improved_forgetting']:
            # åªæœ‰åœ¨æœ‰é—å¿˜æœºåˆ¶æ—¶ï¼ŒTimeré«˜äº†æ‰æ‰£åˆ†
            # è¿™ä¼šé€¼è¿«Agentå»å‹ä½Timerï¼ˆå³è½®æ¢å¤ä¹ ï¼‰
            timer_penalty_coef = 0.5  # ã€EXP15ä¿®å¤ã€‘æé«˜åˆ°0.5ï¼Œè¿«ä½¿ç­–ç•¥å¿…é¡»è½®æ¢
        
        timer_sum = self._compute_timer_sum()
        reward -= timer_penalty_coef * timer_sum  # æ–°å¢Timeræƒ©ç½š

        terminated = self._global_success()
        truncated = bool(self.step_count >= self.max_steps)
        self._last_action = skill_id
        
        # åŠ¨ä½œæ©ç ï¼š72ä¸ªåŠ¨ä½œ
        mask = np.ones((72,), dtype=np.int32)
        for s in range(12):
            if self.gaps[s] < self.tolerance:
                mask[s*6:(s+1)*6] = 0
        
        # ä¿®å¤B: æ‰©å±•è§‚å¯Ÿç©ºé—´åŒ…å«å½’ä¸€åŒ–timer
        timer_obs = self._compute_timer_observation()
        obs = np.concatenate([self.gaps, timer_obs])
        
        # æ”¹è¿›6: è¯Šæ–­æ—¥å¿—
        gap_variance = float(np.var(self.gaps))
        action_freq = np.zeros(72, dtype=np.float32)
        for act in self.action_history:
            action_freq[int(act)] += 1.0
        action_freq = action_freq / (action_freq.sum() + 1e-8)
        
        info = {
            # åŸºç¡€æŒ‡æ ‡
            "sum_prev": prev_sum,
            "sum_curr": curr_sum,
            "skill_id": skill_id,
            "type_id": type_id,
            "delta": delta,
            "noise": noise,
            "action_mask": mask.tolist(),
            "forgetting_mode": self.forgetting_mode,
            
            # å…³é”®è¯Šæ–­æŒ‡æ ‡ï¼ˆä¸åˆ†å±‚ç¯å¢ƒå®Œå…¨ä¸€è‡´ï¼‰
            'gap_variance': gap_variance,  # å­¦ä¹ å‡è¡¡æ€§
            'action_entropy': entropy,  # ç­–ç•¥æ¢ç´¢æ°´å¹³
            'timer_mean': float(np.mean(timer_obs)),  # timerçŠ¶æ€
            'timer_std': float(np.std(timer_obs)),  # timeræ³¢åŠ¨æ€§
            
            # é—å¿˜ç›¸å…³ï¼ˆä¸åˆ†å±‚ç¯å¢ƒå®Œå…¨ä¸€è‡´ï¼‰
            'forgetting_triggers': dict(self.forgetting_trigger_stats),  # å„ç±»åˆ«è§¦å‘æ¬¡æ•°
            'effective_decay_cat': effective_decay_cat,  # å®é™…è¡°å‡å¼ºåº¦
            'steps_since_review_cat': dict(self.steps_since_review_cat),  # è·ç¦»ä¸Šæ¬¡å¤ä¹ 
            'forgetting_event': forgetting_event,  # æœ¬æ¬¡æ˜¯å¦è§¦å‘
            
            # ç­–ç•¥åˆ†æ
            'action_freq': action_freq.tolist(),  # åŠ¨ä½œä½¿ç”¨é¢‘ç‡
            'gap_mean': float(np.mean(self.gaps)),  # å¹³å‡gap
            
            # ä¿ç•™åŸæœ‰å­—æ®µï¼ˆå‘åå…¼å®¹ï¼‰
            "timer_obs": timer_obs.tolist(),
            "mastery_before": mastery_before.tolist(),
            "mastery_after": mastery.tolist(),
        }

        # ã€EXP15ä¿®å¤ã€‘å¼ºåˆ¶ç­–ç•¥å…³æ³¨æ‰€æœ‰ç»´åº¦ - é˜²æ­¢åç¼©
        gap_variance = float(np.var(self.gaps))
        if gap_variance > 0.05:
            reward -= 5.0 * gap_variance  # æƒ©ç½šä¸å‡è¡¡çš„å­¦ä¹ 
        else:
            reward += 0.5  # å¥–åŠ±å‡è¡¡çš„å­¦ä¹ 
        
        # æ›´æ–°å†å²ç¼“å†²åŒº
        for cat, idxs in self.categories.items():
            M_c = float(np.mean(mastery[idxs]))
            hb = self.history_buffer_cat.get(cat, [])
            hb = hb + [M_c]
            if len(hb) > 50:
                hb = hb[-50:]
            self.history_buffer_cat[cat] = hb
            
        return obs, reward, terminated, truncated, info
============================================================

æ–‡ä»¶: scripts\manager_bc_warmup.py
----------------------------------------
# -*- coding: utf-8 -*-
import os
import json
import argparse
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../mini', '..')))

from mini.env.v6_mini_env import MiniManagerEnv12, FlatMiniEnv12


def ensure_dir(p):
    os.makedirs(p, exist_ok=True)


def load_low_policies(low_root: str, dims: int):
    from stable_baselines3 import PPO
    low_policies = []
    for i in range(dims):
        path = os.path.join(low_root, f"skill{i}", "policy.zip")
        if not os.path.exists(path):
            continue
        low_policies.append(PPO.load(path))
    return low_policies


def load_dataset(jsonl_path: str):
    X, y = [], []
    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            rec = json.loads(line)
            X.append(np.array(rec["obs"], dtype=np.float32))
            y.append(int(rec["action"]))
    X = np.stack(X, axis=0)
    y = np.array(y, dtype=np.int64)
    return X, y


def supervised_warmup(model, X: np.ndarray, y: np.ndarray, epochs: int = 8, lr: float = 3e-4, batch_size: int = 256, log_path: str = None):
    """
    æ”¹è¿›3: BC decay - BC lossç³»æ•°éšè®­ç»ƒæ­¥æ•°è¡°å‡è‡³0
    å¢å¼ºæ•°æ®è®°å½•ï¼šè®°å½•BCè®­ç»ƒè¿‡ç¨‹ä¸­çš„ç­–ç•¥æ¼”åŒ–æ•°æ®
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.policy.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = model.policy.optimizer if hasattr(model.policy, "optimizer") else optim.Adam(model.policy.parameters(), lr=lr)

    dataset = torch.utils.data.TensorDataset(torch.from_numpy(X), torch.from_numpy(y))
    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=False)

    model.policy.train()
    
    # æ”¹è¿›3: è®¡ç®—æ€»è®­ç»ƒæ­¥æ•°ç”¨äºBC decay
    total_steps = len(loader) * epochs
    bc_decay_steps = total_steps  # BCåœ¨æ‰€æœ‰æ­¥æ•°å†…è¡°å‡è‡³0
    initial_bc_coef = 1.0
    
    # å¢å¼ºæ•°æ®è®°å½•ï¼šBCè®­ç»ƒè¿‡ç¨‹è®°å½•
    bc_evolution = []
    
    step_count = 0
    for epoch in range(epochs):
        # åŠ¨æ€è·å–åŠ¨ä½œç©ºé—´å¤§å°
        action_space_size = model.env.action_space.n if hasattr(model.env, 'action_space') else 12
        
        epoch_metrics = {
            'epoch': epoch,
            'loss': 0.0,
            'accuracy': 0.0,
            'action_distribution': np.zeros(action_space_size)
        }
        
        for batch_idx, (xb, yb) in enumerate(loader):
            xb = xb.to(device)
            yb = yb.to(device)
            
            # æ”¹è¿›3: è®¡ç®—BCç³»æ•°è¡°å‡
            bc_coef = initial_bc_coef * max(0.0, 1.0 - step_count / bc_decay_steps)
            
            # å‰å‘ï¼šæå–ç‰¹å¾ â†’ pi â†’ logits
            features = model.policy.extract_features(xb)
            latent_pi, latent_vf = model.policy.mlp_extractor(features)
            logits = model.policy.action_net(latent_pi)
            loss = criterion(logits, yb) * bc_coef  # åº”ç”¨è¡°å‡ç³»æ•°
            
            # è®¡ç®—å‡†ç¡®ç‡å’ŒåŠ¨ä½œåˆ†å¸ƒ
            with torch.no_grad():
                pred = torch.argmax(logits, dim=1)
                accuracy = (pred == yb).float().mean().item()
                
                # è®°å½•åŠ¨ä½œåˆ†å¸ƒ
                probs = torch.softmax(logits, dim=-1).cpu().numpy()
                epoch_metrics['action_distribution'] += np.sum(probs, axis=0)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            epoch_metrics['loss'] += loss.item()
            epoch_metrics['accuracy'] += accuracy
            
            # è®°å½•è¯¦ç»†çš„è®­ç»ƒæ•°æ®
            if step_count % 10 == 0:  # æ¯10æ­¥è®°å½•ä¸€æ¬¡
                bc_evolution.append({
                    "step": step_count,
                    "epoch": epoch,
                    "batch": batch_idx,
                    "bc_coef": bc_coef,
                    "loss": loss.item(),
                    "accuracy": accuracy,
                    "lr": optimizer.param_groups[0]['lr'] if hasattr(optimizer, 'param_groups') else lr
                })
            
            step_count += 1
        
        # å½’ä¸€åŒ–åˆ†å¸ƒå’Œè®¡ç®—epochå¹³å‡å€¼
        epoch_metrics['action_distribution'] = (
            epoch_metrics['action_distribution'] / 
            np.sum(epoch_metrics['action_distribution'])
        ).tolist()
        epoch_metrics['loss'] /= len(loader)
        epoch_metrics['accuracy'] /= len(loader)
        epoch_metrics['final_bc_coef'] = bc_coef
        epoch_metrics['total_batches'] = len(loader)
        
        bc_evolution.append(epoch_metrics)
        
        print(f"BC Epoch {epoch+1}/{epochs}: Loss={epoch_metrics['loss']:.4f}, Acc={epoch_metrics['accuracy']:.3f}, BC_coef={bc_coef:.3f}")
    
    # ä¿å­˜BCæ¼”åŒ–æ•°æ®
    if log_path:
        with open(log_path, 'w', encoding='utf-8') as f:
            json.dump(bc_evolution, f, indent=2, ensure_ascii=False)
    
    return model


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--bc-data", type=str, default="mini/data/manager_bc.jsonl")
    parser.add_argument("--low-root", type=str, default="mini/results/low")
    parser.add_argument("--output", type=str, default="mini/results/manager_bc_init")
    parser.add_argument("--seed", type=int, default=777)
    parser.add_argument("--dims", type=int, default=12)
    parser.add_argument("--resource-enabled", action="store_true")
    parser.add_argument("--forgetting-mode", type=str, default='no_forgetting')
    parser.add_argument("--flat-mode", action="store_true", help="Use Flat environment for BC warmup")
    args = parser.parse_args()

    ensure_dir(args.output)
    X, y = load_dataset(args.bc_data)

    dims_data = int(X.shape[1]) if len(X.shape) == 2 else int(args.dims)
    
    if args.flat_mode:
        # Flatæ¨¡å¼ï¼šä¸éœ€è¦ä½å±‚ç­–ç•¥ï¼Œç›´æ¥ä½¿ç”¨FlatMiniEnv12
        env = FlatMiniEnv12(tolerance=0.05, max_steps=200, seed=args.seed, 
                           forgetting_mode=str(args.forgetting_mode), 
                           resource_enabled=bool(args.resource_enabled))
    else:
        # åˆ†å±‚æ¨¡å¼ï¼šéœ€è¦ä½å±‚ç­–ç•¥
        low_policies = load_low_policies(args.low_root, dims_data)
        env = MiniManagerEnv12(low_policies=low_policies, tolerance=0.05, max_steps=200, 
                              seed=args.seed, forgetting_mode=str(args.forgetting_mode), 
                              resource_enabled=bool(args.resource_enabled))

    from stable_baselines3 import PPO
    model = PPO(
        policy="MlpPolicy",
        env=env,
        learning_rate=3e-4,
        n_steps=256,
        batch_size=256,
        n_epochs=10,
        gamma=0.99,
        ent_coef=0.05,
        seed=args.seed,
        verbose=0,
        policy_kwargs=dict(net_arch=[128, 128])  # ä¸ç®¡ç†è€…è®­ç»ƒä¿æŒä¸€è‡´
    )

    # åˆ›å»ºç»“æœç›®å½•ç»“æ„
    mode_dir = os.path.join(args.output, f"manager_{args.forgetting_mode}")  # ç»Ÿä¸€ç›®å½•ç»“æ„
    ensure_dir(mode_dir)
    
    bc_log_path = os.path.join(args.output, f"manager_{args.forgetting_mode}", f"bc_evolution_seed{args.seed}.json")  # ä¸è®­ç»ƒæ›²çº¿åŒç›®å½•
    supervised_warmup(model, X, y, epochs=8, lr=3e-4, batch_size=256, log_path=bc_log_path)
    
    out_path = os.path.join(mode_dir, f"manager_bc_seed{args.seed}.zip")
    model.save(out_path)
    
    # ç”ŸæˆBCé˜¶æ®µæ±‡æ€»
    bc_summary = {
        "forgetting_mode": args.forgetting_mode,
        "seed": args.seed,
        "total_epochs": 8,
        "total_samples": len(X),
        "bc_decay_applied": True,
        "flat_mode": args.flat_mode
    }
    
    summary_file = os.path.join(mode_dir, f"bc_summary_{args.forgetting_mode}_seed{args.seed}.json")
    with open(summary_file, "w", encoding="utf-8") as f:
        json.dump(bc_summary, f, indent=2, ensure_ascii=False)
    
    print(f"BCé¢„çƒ­å®Œæˆï¼æ¨¡å‹ä¿å­˜è‡³: {out_path}")
    print(f"è®­ç»ƒæ—¥å¿—ä¿å­˜è‡³: {bc_log_path}")


if __name__ == "__main__":
    main()

============================================================

æ–‡ä»¶: train\train_low_level.py
----------------------------------------
# -*- coding: utf-8 -*-
import os
import json
import argparse
import numpy as np
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../mini', '..')))

from mini.env.v6_mini_env import MiniSkillEnv


def ensure_dir(p):
    os.makedirs(p, exist_ok=True)


def train_one_skill(skill_id: int, steps: int, ent: float, out_dir: str, seed: int = 777):
    from stable_baselines3 import PPO
    from stable_baselines3.common.callbacks import BaseCallback
    import torch.nn as nn

    # ã€å…³é”®1ã€‘æ¯ä¸ªæŠ€èƒ½ç‹¬ç«‹ä½†ç¡®å®šçš„ç§å­
    unique_seed = 777 + skill_id * 1000  # ééšæœºï¼ç¡®ä¿å¯é‡å¤æ€§
    
    env = MiniSkillEnv(tolerance=0.05, max_steps=200, seed=unique_seed)
    ensure_dir(out_dir)

    class CurveCallback(BaseCallback):
        def __init__(self):
            super().__init__()
            self.rows = []

        def _on_step(self) -> bool:
            # ä½¿ç”¨ ep_info_buffer æå–å¹³å‡å¥–åŠ±ä¸é•¿åº¦
            try:
                ep_info = self.model.ep_info_buffer
                if len(ep_info) > 0:
                    r = np.mean([x['r'] for x in ep_info])
                    l = np.mean([x['l'] for x in ep_info])
                else:
                    r, l = 0.0, 0.0
                self.rows.append({"step": int(self.num_timesteps), "ep_rew_mean": float(r), "ep_len_mean": float(l)})
            except Exception:
                pass
            return True

    # ã€ä¿®å¤9ã€‘è°ƒæ•´è®­ç»ƒå‚æ•° - æ”¹ä¸ºå•é˜¶æ®µ1500æ­¥ï¼Œent=0.08
    model = PPO(
        policy="MlpPolicy",
        env=env,
        learning_rate=3e-4,        # ä¿æŒå­¦ä¹ ç‡
        n_steps=2048,              # ä¿æŒé‡‡æ ·æ­¥æ•°
        batch_size=64,             # ä¿æŒbatch size
        n_epochs=4,                # ä¿æŒepochæ•°
        gamma=0.99,
        ent_coef=0.2,              # ã€EXP15ä¿®å¤ã€‘å¼ºåˆ¶æ¢ç´¢ï¼Œé˜²æ­¢åŠ¨ä½œå•ä¸€
        seed=unique_seed,
        verbose=0,
        policy_kwargs=dict(
            net_arch=[128, 128],   # ä¿æŒç½‘ç»œå®¹é‡
            activation_fn=nn.Tanh,
            ortho_init=True
        )
    )
    
    # ã€EXP15ä¿®å¤ã€‘æ”¹ä¸º3000æ­¥è®­ç»ƒï¼Œå……åˆ†æ¢ç´¢
    print(f"è®­ç»ƒæŠ€èƒ½{skill_id}: å•é˜¶æ®µè®­ç»ƒ3000æ­¥")
    cb = CurveCallback()
    model.learn(total_timesteps=3000, callback=cb, reset_num_timesteps=False)
    
    # ã€ä¿®å¤11ã€‘ç«‹å³è¯„ä¼°å¹¶ä¿å­˜
    success_count = 0
    action_dist = np.zeros(6)
    
    for _ in range(30):  # 30ä¸ªepisodeè¯„ä¼°
        obs, _ = env.reset()
        done = False
        while not done:
            act, _ = model.predict(obs, deterministic=True)
            action_dist[act] += 1
            obs, _, terminated, truncated, _ = env.step(int(act))
            done = terminated or truncated
            if terminated:
                success_count += 1
    
    success_rate = success_count / 30.0
    
    # ã€ä¿®å¤12ã€‘åªæœ‰åŠ¨ä½œå¤šæ ·ä¸”æˆåŠŸç‡é«˜æ‰ä¿å­˜
    action_dist_norm = action_dist / (action_dist.sum() + 1e-8)
    action_entropy = -np.sum(action_dist_norm * np.log(action_dist_norm + 1e-8))
    
    print(f"æŠ€èƒ½{skill_id}è¯„ä¼°: æˆåŠŸç‡={success_rate:.2%}, åŠ¨ä½œç†µ={action_entropy:.3f}")
    print(f"  åŠ¨ä½œåˆ†å¸ƒ: {action_dist}")
    
    # ã€EXP15ä¿®å¤ã€‘åªè¦æˆåŠŸå°±ä¿å­˜ï¼Œé™ä½åç¼©æ¨¡å‹è¢«ä¸¢å¼ƒçš„æ¦‚ç‡
    if success_rate > 0.5:  # ä¸»è¦é é«˜ç†µå’Œé«˜æ­¥æ•°æ¥å‡å°‘åç¼©å‘ç”Ÿ
        model.save(os.path.join(out_dir, "policy.zip"))
        print(f"æŠ€èƒ½{skill_id}ä¿å­˜æˆåŠŸ")
    else:
        print(f"è­¦å‘Š: æŠ€èƒ½{skill_id}æˆåŠŸç‡è¿‡ä½ï¼Œä½†ä»ç„¶ä¿å­˜...")
        # å³ä½¿æˆåŠŸç‡ä½ä¹Ÿä¿å­˜ï¼Œé¿å…è®­ç»ƒå¤±è´¥
        model.save(os.path.join(out_dir, "policy.zip"))

    # ä¿å­˜è®­ç»ƒæ›²çº¿å’Œè¯„ä¼°æŒ‡æ ‡
    with open(os.path.join(out_dir, "training_curve.json"), "w", encoding="utf-8") as f:
        json.dump(cb.rows, f, indent=2, ensure_ascii=False)

    metrics = {
        "skill_id": skill_id,
        "success_rate": success_rate,
        "action_entropy": action_entropy,
        "action_dist": action_dist.tolist()
    }
    with open(os.path.join(out_dir, "eval_metrics.json"), "w", encoding="utf-8") as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--steps", type=int, default=10000)

    parser.add_argument("--ent", type=float, default=0.01)
    parser.add_argument("--output", type=str, default="mini/results/low")
    parser.add_argument("--seed", type=int, default=777)
    parser.add_argument("--num-skills", type=int, default=12)
    args = parser.parse_args()

    ensure_dir(args.output)
    for skill_id in range(int(args.num_skills)):
        out_dir = os.path.join(args.output, f"skill{skill_id}")
        train_one_skill(skill_id, args.steps, args.ent, out_dir, seed=args.seed)


if __name__ == "__main__":
    main()

============================================================

æ–‡ä»¶: train\train_manager.py
----------------------------------------
# -*- coding: utf-8 -*-
import os
import json
import argparse
import numpy as np
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../mini', '..')))

from mini.env.v6_mini_env import MiniManagerEnv12, FlatMiniEnv12


def ensure_dir(p):
    os.makedirs(p, exist_ok=True)


def load_low_policies(low_root: str, dims: int):
    from stable_baselines3 import PPO
    low_policies = []
    missing = []
    print(f"  å¼€å§‹åŠ è½½åº•å±‚ç­–ç•¥ï¼Œè·¯å¾„: {low_root}")
    
    for i in range(int(dims)):
        policy_path = os.path.join(low_root, f"skill{i}", "policy.zip")
        if os.path.exists(policy_path):
            try:
                policy = PPO.load(policy_path)
                low_policies.append(policy)
                print(f"  æˆåŠŸåŠ è½½ skill{i}: {policy_path}")
            except Exception as e:
                print(f"  åŠ è½½ skill{i} å¤±è´¥: {e}")
                missing.append(policy_path)
        else:
            print(f"  æ–‡ä»¶ä¸å­˜åœ¨: {policy_path}")
            missing.append(policy_path)
    
    print(f"  æ€»è®¡åŠ è½½ {len(low_policies)}/{dims} ä¸ªåº•å±‚ç­–ç•¥")
    
    if int(dims) == 12 and len(low_policies) != int(dims):
        print(f"  é”™è¯¯ï¼šç¼ºå¤±çš„ç­–ç•¥æ–‡ä»¶: {missing}")
        raise FileNotFoundError(f"Expected 12 low-level PPO policies; missing: {missing}")
    
    return low_policies


def train_manager(steps: int, ent: float, out_dir: str, low_root: str, seed: int = 777, max_steps: int = 200, init_from: str | None = None, dims: int = 4, forgetting_mode: str = 'no_forgetting', resource_enabled: bool = False, resource_decay_range: tuple | list = (0.2, 0.4), match_bonus: float = 0.2, mismatch_penalty: float = 0.1, timing_bonus: float = 0.03, timing_penalty: float = 0.03, difficulty_bins: tuple | list = (0.33, 0.66), flat_mode: bool = False):
    from stable_baselines3 import PPO
    from stable_baselines3.common.callbacks import BaseCallback
    from stable_baselines3.common.vec_env import DummyVecEnv

    ensure_dir(out_dir)
    
    # ä¿®å¤ï¼šé¢„å…ˆåŠ è½½ä½å±‚ç­–ç•¥ï¼Œé¿å…åœ¨ç¯å¢ƒåˆ›å»ºæ—¶ä¸¢å¤±
    low_policies = None
    if not flat_mode:
        low_policies = load_low_policies(low_root, dims)
        print(f"  æˆåŠŸåŠ è½½ {len(low_policies)} ä¸ªä½å±‚ç­–ç•¥ç”¨äºè®­ç»ƒ")
    
    # ä¿®å¤ï¼šåˆ›å»ºç¯å¢ƒå·¥å‚å‡½æ•°ï¼Œç¡®ä¿low_policiesæ­£ç¡®ä¼ é€’
    def make_env():
        if flat_mode:
            return FlatMiniEnv12(
                tolerance=0.05,
                max_steps=max_steps,
                seed=seed,
                forgetting_mode=str(forgetting_mode),
                resource_enabled=bool(resource_enabled),
                resource_decay_range=tuple(resource_decay_range),
                match_bonus=float(match_bonus),
                mismatch_penalty=float(mismatch_penalty),
                timing_bonus=float(timing_bonus),
                timing_penalty=float(timing_penalty),
                difficulty_bins=tuple(difficulty_bins)
            )
        else:
            return MiniManagerEnv12(
                low_policies=low_policies,  # ä½¿ç”¨é¢„åŠ è½½çš„ç­–ç•¥
                tolerance=0.05,
                max_steps=max_steps,
                seed=seed,
                forgetting_mode=str(forgetting_mode),
                resource_enabled=bool(resource_enabled),
                resource_decay_range=tuple(resource_decay_range),
                match_bonus=float(match_bonus),
                mismatch_penalty=float(mismatch_penalty),
                timing_bonus=float(timing_bonus),
                timing_penalty=float(timing_penalty),
                difficulty_bins=tuple(difficulty_bins)
            )
    
    # ä½¿ç”¨DummyVecEnvåŒ…è£…ç¯å¢ƒï¼Œç¡®ä¿å…¼å®¹æ€§
    env = DummyVecEnv([make_env])
    
    print(f"  ç¯å¢ƒåˆ›å»ºæˆåŠŸï¼Œç±»å‹: DummyVecEnv")
    if not flat_mode:
        print(f"  ç¯å¢ƒä¸­çš„ä½å±‚ç­–ç•¥æ•°é‡: {len(low_policies)}")

    # ã€ä¿®å¤22ã€‘æ·»åŠ è®­ç»ƒå›è°ƒç›‘æ§åç¼©
    class AntiCollapseCallback(BaseCallback):
        def __init__(self, check_freq=1000):
            super().__init__()
            self.check_freq = check_freq
            self.action_counts = np.zeros(env.envs[0].action_space.n)
            
        def _on_step(self):
            if self.n_calls % self.check_freq == 0:
                # æ£€æŸ¥åŠ¨ä½œåˆ†å¸ƒ
                if len(self.model.ep_info_buffer) > 0:
                    recent_actions = self.model.rollout_buffer.actions[-100:] if hasattr(self.model.rollout_buffer, 'actions') else []
                    if len(recent_actions) > 0:
                        unique_actions = np.unique(recent_actions)
                        diversity = len(unique_actions) / env.envs[0].action_space.n
                        if diversity < 0.3:  # å¦‚æœåŠ¨ä½œå¤šæ ·æ€§ä½äº30%
                            print(f"è­¦å‘Šï¼šåŠ¨ä½œå¤šæ ·æ€§ä½ ({diversity:.2%})ï¼Œå¢åŠ ç†µç³»æ•°")
                            self.model.ent_coef = min(0.15, self.model.ent_coef * 1.5)
            return True

    class CurveCallback(BaseCallback):
        def __init__(self, log_path, forgetting_mode, seed):
            super().__init__()
            self.rows = []
            self.log_path = log_path
            self.forgetting_mode = forgetting_mode
            self.seed = seed
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(log_path), exist_ok=True)
        
        def _on_step(self) -> bool:
            if len(self.model.ep_info_buffer) > 0:
                # åŸºç¡€æŒ‡æ ‡
                ep_info = self.model.ep_info_buffer[-1]
                r = ep_info['r']
                l = ep_info['l']
                
                # ä»infoå­—å…¸æå–è¯Šæ–­æŒ‡æ ‡
                info = ep_info.get('info', {})
                diag = {
                    'gap_variance': info.get('gap_variance', 0.0),
                    'action_entropy': info.get('action_entropy', 0.0),
                    'timer_mean': info.get('timer_mean', 0.0),
                    'timer_std': info.get('timer_std', 0.0),
                    'forgetting_triggers': info.get('forgetting_triggers', {}),
                    'effective_decay_cat': info.get('effective_decay_cat', {}),
                    'steps_since_review_cat': info.get('steps_since_review_cat', {}),
                    'action_freq': info.get('action_freq', []),
                    'forgetting_events': {cat: bool(v) for cat, v in info.get('forgetting_event', {}).items()}
                }
                
                # åˆå¹¶è®°å½•
                self.rows.append({
                    "step": int(self.num_timesteps),
                    "ep_rew_mean": float(r),
                    "ep_len_mean": float(l),
                    "forgetting_mode": self.forgetting_mode,
                    "seed": self.seed,
                    **diag
                })
            return True
        
        def _on_training_end(self):
            # ä¿å­˜å®Œæ•´è®­ç»ƒæ›²çº¿
            with open(self.log_path, "w", encoding="utf-8") as f:
                json.dump(self.rows, f, indent=2, ensure_ascii=False)
            
            # è‡ªåŠ¨è¿è¡Œæœ€ç»ˆè¯„ä¼°
            self._run_final_evaluation()
        
        def _run_final_evaluation(self):
            """è®­ç»ƒç»“æŸåè‡ªåŠ¨è¿è¡Œ50 episodeè¯„ä¼°"""
            env = self.training_env.envs[0]
            model = self.model
            
            eval_results = {
                'nf_eval': {'success_rate': 0, 'avg_steps': 0, 'metrics': {}},
                'ff_eval': {'success_rate': 0, 'avg_steps': 0, 'metrics': {}},
                'if_eval': {'success_rate': 0, 'avg_steps': 0, 'metrics': {}}
            }
            
            for eval_mode in ['no_forgetting', 'fixed_forgetting', 'improved_forgetting']:
                env.forgetting_mode = eval_mode
                successes = 0
                total_steps = 0
                metrics_accum = {
                    'gap_variance': [],
                    'forgetting_triggers': {'algebra':0,'geometry':0,'statistics':0}
                }
                
                for _ in range(50):
                    obs, info = env.reset()
                    done = False
                    steps = 0
                    
                    while not done and steps < env.max_steps:
                        action, _ = model.predict(obs, deterministic=True)
                        obs, reward, terminated, truncated, info = env.step(action)
                        done = terminated or truncated
                        steps += 1
                    
                    successes += 1 if terminated else 0
                    total_steps += steps
                    
                    # æ”¶é›†æŒ‡æ ‡
                    metrics_accum['gap_variance'].append(info.get('gap_variance', 0.0))
                    for cat in ['algebra','geometry','statistics']:
                        metrics_accum['forgetting_triggers'][cat] += info.get('forgetting_triggers', {}).get(cat, 0)
                
                # è®¡ç®—å¹³å‡å€¼
                key = f"{eval_mode[:2]}_eval"
                eval_results[key] = {
                    'success_rate': successes / 50.0,
                    'avg_steps': total_steps / 50.0,
                    'metrics': {
                        'avg_gap_variance': np.mean(metrics_accum['gap_variance']),
                        'total_forgetting_triggers': metrics_accum['forgetting_triggers']
                    }
                }
            
            eval_path = os.path.join(os.path.dirname(self.log_path), f"final_evaluation_{self.forgetting_mode}_seed{self.seed}.json")
            with open(eval_path, 'w', encoding='utf-8') as f:
                json.dump(eval_results, f, indent=2, ensure_ascii=False)

    # ã€ä¿®å¤20ã€‘è°ƒæ•´ç½‘ç»œæ¶æ„å’Œè®­ç»ƒå‚æ•°
    if flat_mode:
        policy_kwargs = dict(net_arch=[256, 256])  # å‡å°‘ä¸€å±‚
        effective_ent = max(0.30, ent)  # ã€EXP15ä¿®å¤ã€‘æé«˜ç†µï¼Œå¼ºåˆ¶æ¢ç´¢ï¼ˆæ‰å¹³ï¼‰
    else:
        policy_kwargs = dict(net_arch=[128, 128])  # é€‚ä¸­å¤§å°
        effective_ent = max(0.20, ent)  # ã€EXP15ä¿®å¤ã€‘æé«˜ç†µï¼Œå¼ºåˆ¶æ¢ç´¢ï¼ˆåˆ†å±‚ï¼‰

    if init_from and os.path.exists(init_from):
        # ä¿®å¤ï¼šå…ˆåˆ›å»ºæ¨¡å‹ï¼Œå†åŠ è½½å‚æ•°ï¼Œé¿å…ç¯å¢ƒé‡æ–°åŒ…è£…
        # ã€ä¿®å¤21ã€‘ä½¿ç”¨æ›´é«˜çš„åˆå§‹ç†µç³»æ•°
        model = PPO(
            policy="MlpPolicy",
            env=env,
            learning_rate=2e-4,
            n_steps=512,          # å¢åŠ é‡‡æ ·é¢‘ç‡
            batch_size=128,
            n_epochs=8,
            gamma=0.99,
            ent_coef=effective_ent,  # ä½¿ç”¨è°ƒæ•´åçš„ç†µç³»æ•°
            seed=seed,
            verbose=1,            # æ˜¾ç¤ºè®­ç»ƒæ—¥å¿—
            policy_kwargs=policy_kwargs
        )
        
        # åŠ è½½é¢„è®­ç»ƒçš„å‚æ•°
        pretrained_model = PPO.load(init_from)
        model.policy.load_state_dict(pretrained_model.policy.state_dict())
        print(f"  ä» {init_from} åŠ è½½ç­–ç•¥å‚æ•°åˆ°æ–°æ¨¡å‹")
    else:
        model = PPO(
            policy="MlpPolicy",
            env=env,
            learning_rate=2e-4,
            n_steps=512,          # å¢åŠ é‡‡æ ·é¢‘ç‡
            batch_size=128,
            n_epochs=8,
            gamma=0.99,
            ent_coef=effective_ent,  # ä½¿ç”¨è°ƒæ•´åçš„ç†µç³»æ•°
            seed=seed,
            verbose=1,            # æ˜¾ç¤ºè®­ç»ƒæ—¥å¿—
            policy_kwargs=policy_kwargs
        )
        print(f"  åˆ›å»ºæ–°çš„PPOæ¨¡å‹")

    log_dir = os.path.join(out_dir, f"training_curve_{forgetting_mode}_seed{seed}.json")
    cb = CurveCallback(
        log_path=log_dir,
        forgetting_mode=forgetting_mode,
        seed=seed
    )
    
    # ã€EXP15ä¿®å¤ã€‘å•é˜¶æ®µè®­ç»ƒï¼Œå…¨ç¨‹é«˜ç†µï¼Œé˜²æ­¢åç¼©
    if flat_mode:
        # FLAT_IFæ¨¡å‹ï¼šå…¨ç¨‹é«˜ç†µ
        stages = [
            (14500, effective_ent),  # å…¨ç¨‹é«˜ç†µ
        ]
    else:
        # åˆ†å±‚æ¨¡å‹ï¼šå…¨ç¨‹é«˜ç†µ
        stages = [
            (13000, effective_ent),  # å…¨ç¨‹é«˜ç†µ
        ]
    
    total_trained = 0
    for stage_steps, stage_ent in stages:
        model.ent_coef = stage_ent
        print(f"è®­ç»ƒé˜¶æ®µ: {stage_steps}æ­¥, ç†µç³»æ•°={stage_ent:.3f}")
        model.learn(
            total_timesteps=stage_steps,
            callback=[AntiCollapseCallback(), cb],
            reset_num_timesteps=False
        )
        total_trained += stage_steps
        
        # é˜¶æ®µè¯„ä¼°
        if not flat_mode:
            # ç®€å•è¯„ä¼°
            env_single = env.envs[0]
            success_count = 0
            for _ in range(20):
                obs, _ = env_single.reset()
                done = False
                steps = 0
                while not done and steps < max_steps:
                    action, _ = model.predict(obs, deterministic=True)
                    obs, reward, terminated, truncated, info = env_single.step(action)
                    done = terminated or truncated
                    steps += 1
                success_count += 1 if terminated else 0
            
            success_rate = success_count / 20.0
            print(f"é˜¶æ®µå®Œæˆï¼Œå¿«é€Ÿè¯„ä¼°æˆåŠŸç‡: {success_rate:.2%}")
            if success_rate < 0.1 and total_trained > 5000:
                print("è­¦å‘Šï¼šæˆåŠŸç‡è¿‡ä½ï¼Œè€ƒè™‘è°ƒæ•´")

    # åˆ›å»ºç»“æœç›®å½•ç»“æ„
    mode_dir = os.path.join(out_dir, f"manager_{forgetting_mode}")
    ensure_dir(mode_dir)
    
    model.save(os.path.join(mode_dir, f"manager_policy_seed{seed}.zip"))
    
    print(f"è®­ç»ƒå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {mode_dir}")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--steps", type=int, default=10000)
    parser.add_argument("--ent", type=float, default=0.02)
    parser.add_argument("--output", type=str, default="mini/results/manager")
    parser.add_argument("--low-root", type=str, default="mini/results/low")
    parser.add_argument("--seed", type=int, default=777)
    parser.add_argument("--max-steps", type=int, default=800)  # ä¿®æ”¹é»˜è®¤å€¼ä¸º800
    parser.add_argument("--init-from", type=str, default=None)
    parser.add_argument("--dims", type=int, default=12)
    parser.add_argument("--forgetting-mode", type=str, default='no_forgetting')
    parser.add_argument("--resource-enabled", action="store_true")
    parser.add_argument("--resource-decay-range", type=float, nargs=2, default=[0.2, 0.4])
    parser.add_argument("--match-bonus", type=float, default=0.2)
    parser.add_argument("--mismatch-penalty", type=float, default=0.1)
    parser.add_argument("--timing-bonus", type=float, default=0.03)
    parser.add_argument("--timing-penalty", type=float, default=0.03)
    parser.add_argument("--difficulty-bins", type=float, nargs=2, default=[0.33, 0.66])
    parser.add_argument("--flat-mode", action="store_true", help="Use Flat environment (no hierarchy)")
    args = parser.parse_args()

    ensure_dir(args.output)
    train_manager(
        args.steps,
        args.ent,
        args.output,
        args.low_root,
        seed=args.seed,
        max_steps=args.max_steps,
        init_from=args.init_from,
        dims=args.dims,
        forgetting_mode=args.forgetting_mode,
        resource_enabled=bool(args.resource_enabled),
        resource_decay_range=tuple(args.resource_decay_range),
        match_bonus=args.match_bonus,
        mismatch_penalty=args.mismatch_penalty,
        timing_bonus=args.timing_bonus,
        timing_penalty=args.timing_penalty,
        difficulty_bins=tuple(args.difficulty_bins),
        flat_mode=args.flat_mode
    )


if __name__ == "__main__":
    main()
